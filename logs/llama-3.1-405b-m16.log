{'dim': 16384, 'ffn_dim_multiplier': 1.2, 'multiple_of': 4096, 'n_heads': 128, 'n_kv_heads': 16, 'n_layers': 126, 'norm_eps': 1e-05, 'rope_theta': 500000.0, 'use_scaled_rope': True, 'vocab_size': 128256}
DIM= 16384
000: consolidated.00/consolidated-00001-of-00011.pth
001: consolidated.00/consolidated-00002-of-00011.pth
002: consolidated.00/consolidated-00003-of-00011.pth
003: consolidated.00/consolidated-00004-of-00011.pth
004: consolidated.00/consolidated-00005-of-00011.pth
005: consolidated.00/consolidated-00006-of-00011.pth
006: consolidated.00/consolidated-00007-of-00011.pth
007: consolidated.00/consolidated-00008-of-00011.pth
008: consolidated.00/consolidated-00009-of-00011.pth
009: consolidated.00/consolidated-00010-of-00011.pth
010: consolidated.00/consolidated-00011-of-00011.pth
011: consolidated.01/consolidated-00001-of-00011.pth
012: consolidated.01/consolidated-00002-of-00011.pth
013: consolidated.01/consolidated-00003-of-00011.pth
014: consolidated.01/consolidated-00004-of-00011.pth
015: consolidated.01/consolidated-00005-of-00011.pth
016: consolidated.01/consolidated-00006-of-00011.pth
017: consolidated.01/consolidated-00007-of-00011.pth
018: consolidated.01/consolidated-00008-of-00011.pth
019: consolidated.01/consolidated-00009-of-00011.pth
020: consolidated.01/consolidated-00010-of-00011.pth
021: consolidated.01/consolidated-00011-of-00011.pth
022: consolidated.02/consolidated-00001-of-00011.pth
023: consolidated.02/consolidated-00002-of-00011.pth
024: consolidated.02/consolidated-00003-of-00011.pth
025: consolidated.02/consolidated-00004-of-00011.pth
026: consolidated.02/consolidated-00005-of-00011.pth
027: consolidated.02/consolidated-00006-of-00011.pth
028: consolidated.02/consolidated-00007-of-00011.pth
029: consolidated.02/consolidated-00008-of-00011.pth
030: consolidated.02/consolidated-00009-of-00011.pth
031: consolidated.02/consolidated-00010-of-00011.pth
032: consolidated.02/consolidated-00011-of-00011.pth
033: consolidated.03/consolidated-00001-of-00011.pth
034: consolidated.03/consolidated-00002-of-00011.pth
035: consolidated.03/consolidated-00003-of-00011.pth
036: consolidated.03/consolidated-00004-of-00011.pth
037: consolidated.03/consolidated-00005-of-00011.pth
038: consolidated.03/consolidated-00006-of-00011.pth
039: consolidated.03/consolidated-00007-of-00011.pth
040: consolidated.03/consolidated-00008-of-00011.pth
041: consolidated.03/consolidated-00009-of-00011.pth
042: consolidated.03/consolidated-00010-of-00011.pth
043: consolidated.03/consolidated-00011-of-00011.pth
044: consolidated.04/consolidated-00001-of-00011.pth
045: consolidated.04/consolidated-00002-of-00011.pth
046: consolidated.04/consolidated-00003-of-00011.pth
047: consolidated.04/consolidated-00004-of-00011.pth
048: consolidated.04/consolidated-00005-of-00011.pth
049: consolidated.04/consolidated-00006-of-00011.pth
050: consolidated.04/consolidated-00007-of-00011.pth
051: consolidated.04/consolidated-00008-of-00011.pth
052: consolidated.04/consolidated-00009-of-00011.pth
053: consolidated.04/consolidated-00010-of-00011.pth
054: consolidated.04/consolidated-00011-of-00011.pth
055: consolidated.05/consolidated-00001-of-00011.pth
056: consolidated.05/consolidated-00002-of-00011.pth
057: consolidated.05/consolidated-00003-of-00011.pth
058: consolidated.05/consolidated-00004-of-00011.pth
059: consolidated.05/consolidated-00005-of-00011.pth
060: consolidated.05/consolidated-00006-of-00011.pth
061: consolidated.05/consolidated-00007-of-00011.pth
062: consolidated.05/consolidated-00008-of-00011.pth
063: consolidated.05/consolidated-00009-of-00011.pth
064: consolidated.05/consolidated-00010-of-00011.pth
065: consolidated.05/consolidated-00011-of-00011.pth
066: consolidated.06/consolidated-00001-of-00011.pth
067: consolidated.06/consolidated-00002-of-00011.pth
068: consolidated.06/consolidated-00003-of-00011.pth
069: consolidated.06/consolidated-00004-of-00011.pth
070: consolidated.06/consolidated-00005-of-00011.pth
071: consolidated.06/consolidated-00006-of-00011.pth
072: consolidated.06/consolidated-00007-of-00011.pth
073: consolidated.06/consolidated-00008-of-00011.pth
074: consolidated.06/consolidated-00009-of-00011.pth
075: consolidated.06/consolidated-00010-of-00011.pth
076: consolidated.06/consolidated-00011-of-00011.pth
077: consolidated.07/consolidated-00001-of-00011.pth
078: consolidated.07/consolidated-00002-of-00011.pth
079: consolidated.07/consolidated-00003-of-00011.pth
080: consolidated.07/consolidated-00004-of-00011.pth
081: consolidated.07/consolidated-00005-of-00011.pth
082: consolidated.07/consolidated-00006-of-00011.pth
083: consolidated.07/consolidated-00007-of-00011.pth
084: consolidated.07/consolidated-00008-of-00011.pth
085: consolidated.07/consolidated-00009-of-00011.pth
086: consolidated.07/consolidated-00010-of-00011.pth
087: consolidated.07/consolidated-00011-of-00011.pth
088: consolidated.08/consolidated-00001-of-00011.pth
089: consolidated.08/consolidated-00002-of-00011.pth
090: consolidated.08/consolidated-00003-of-00011.pth
091: consolidated.08/consolidated-00004-of-00011.pth
092: consolidated.08/consolidated-00005-of-00011.pth
093: consolidated.08/consolidated-00006-of-00011.pth
094: consolidated.08/consolidated-00007-of-00011.pth
095: consolidated.08/consolidated-00008-of-00011.pth
096: consolidated.08/consolidated-00009-of-00011.pth
097: consolidated.08/consolidated-00010-of-00011.pth
098: consolidated.08/consolidated-00011-of-00011.pth
099: consolidated.09/consolidated-00001-of-00011.pth
100: consolidated.09/consolidated-00002-of-00011.pth
101: consolidated.09/consolidated-00003-of-00011.pth
102: consolidated.09/consolidated-00004-of-00011.pth
103: consolidated.09/consolidated-00005-of-00011.pth
104: consolidated.09/consolidated-00006-of-00011.pth
105: consolidated.09/consolidated-00007-of-00011.pth
106: consolidated.09/consolidated-00008-of-00011.pth
107: consolidated.09/consolidated-00009-of-00011.pth
108: consolidated.09/consolidated-00010-of-00011.pth
109: consolidated.09/consolidated-00011-of-00011.pth
110: consolidated.10/consolidated-00001-of-00011.pth
111: consolidated.10/consolidated-00002-of-00011.pth
112: consolidated.10/consolidated-00003-of-00011.pth
113: consolidated.10/consolidated-00004-of-00011.pth
114: consolidated.10/consolidated-00005-of-00011.pth
115: consolidated.10/consolidated-00006-of-00011.pth
116: consolidated.10/consolidated-00007-of-00011.pth
117: consolidated.10/consolidated-00008-of-00011.pth
118: consolidated.10/consolidated-00009-of-00011.pth
119: consolidated.10/consolidated-00010-of-00011.pth
120: consolidated.10/consolidated-00011-of-00011.pth
121: consolidated.11/consolidated-00001-of-00011.pth
122: consolidated.11/consolidated-00002-of-00011.pth
123: consolidated.11/consolidated-00003-of-00011.pth
124: consolidated.11/consolidated-00004-of-00011.pth
125: consolidated.11/consolidated-00005-of-00011.pth
126: consolidated.11/consolidated-00006-of-00011.pth
127: consolidated.11/consolidated-00007-of-00011.pth
128: consolidated.11/consolidated-00008-of-00011.pth
129: consolidated.11/consolidated-00009-of-00011.pth
130: consolidated.11/consolidated-00010-of-00011.pth
131: consolidated.11/consolidated-00011-of-00011.pth
132: consolidated.12/consolidated-00001-of-00011.pth
133: consolidated.12/consolidated-00002-of-00011.pth
134: consolidated.12/consolidated-00003-of-00011.pth
135: consolidated.12/consolidated-00004-of-00011.pth
136: consolidated.12/consolidated-00005-of-00011.pth
137: consolidated.12/consolidated-00006-of-00011.pth
138: consolidated.12/consolidated-00007-of-00011.pth
139: consolidated.12/consolidated-00008-of-00011.pth
140: consolidated.12/consolidated-00009-of-00011.pth
141: consolidated.12/consolidated-00010-of-00011.pth
142: consolidated.12/consolidated-00011-of-00011.pth
143: consolidated.13/consolidated-00001-of-00011.pth
144: consolidated.13/consolidated-00002-of-00011.pth
145: consolidated.13/consolidated-00003-of-00011.pth
146: consolidated.13/consolidated-00004-of-00011.pth
147: consolidated.13/consolidated-00005-of-00011.pth
148: consolidated.13/consolidated-00006-of-00011.pth
149: consolidated.13/consolidated-00007-of-00011.pth
150: consolidated.13/consolidated-00008-of-00011.pth
151: consolidated.13/consolidated-00009-of-00011.pth
152: consolidated.13/consolidated-00010-of-00011.pth
153: consolidated.13/consolidated-00011-of-00011.pth
154: consolidated.14/consolidated-00001-of-00011.pth
155: consolidated.14/consolidated-00002-of-00011.pth
156: consolidated.14/consolidated-00003-of-00011.pth
157: consolidated.14/consolidated-00004-of-00011.pth
158: consolidated.14/consolidated-00005-of-00011.pth
159: consolidated.14/consolidated-00006-of-00011.pth
160: consolidated.14/consolidated-00007-of-00011.pth
161: consolidated.14/consolidated-00008-of-00011.pth
162: consolidated.14/consolidated-00009-of-00011.pth
163: consolidated.14/consolidated-00010-of-00011.pth
164: consolidated.14/consolidated-00011-of-00011.pth
165: consolidated.15/consolidated-00001-of-00011.pth
166: consolidated.15/consolidated-00002-of-00011.pth
167: consolidated.15/consolidated-00003-of-00011.pth
168: consolidated.15/consolidated-00004-of-00011.pth
169: consolidated.15/consolidated-00005-of-00011.pth
170: consolidated.15/consolidated-00006-of-00011.pth
171: consolidated.15/consolidated-00007-of-00011.pth
172: consolidated.15/consolidated-00008-of-00011.pth
173: consolidated.15/consolidated-00009-of-00011.pth
174: consolidated.15/consolidated-00010-of-00011.pth
175: consolidated.15/consolidated-00011-of-00011.pth

-----------------------------------------------------------------------------
0 params in total.
0 bytes in total.
-----------------------------------------------------------------------------

[1/176]: Loading consolidated.00/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
2493087744 params in total.
4986175488 bytes in total.
1.50 sec, 1.50 sec, 3162.97 MB/s
-----------------------------------------------------------------------------

[2/176]: Loading consolidated.00/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
4984930304 params in total.
9969860608 bytes in total.
3.09 sec, 1.59 sec, 3072.96 MB/s
-----------------------------------------------------------------------------

[3/176]: Loading consolidated.00/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
7472545792 params in total.
14945091584 bytes in total.
4.77 sec, 1.67 sec, 2990.72 MB/s
-----------------------------------------------------------------------------

[4/176]: Loading consolidated.00/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
9943384064 params in total.
19886768128 bytes in total.
6.46 sec, 1.69 sec, 2936.66 MB/s
-----------------------------------------------------------------------------

[5/176]: Loading consolidated.00/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
12435226624 params in total.
24870453248 bytes in total.
8.05 sec, 1.59 sec, 2946.85 MB/s
-----------------------------------------------------------------------------

[6/176]: Loading consolidated.00/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
14922842112 params in total.
29845684224 bytes in total.
9.72 sec, 1.67 sec, 2928.71 MB/s
-----------------------------------------------------------------------------

[7/176]: Loading consolidated.00/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
17393680384 params in total.
34787360768 bytes in total.
11.38 sec, 1.66 sec, 2915.42 MB/s
-----------------------------------------------------------------------------

[8/176]: Loading consolidated.00/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
19885522944 params in total.
39771045888 bytes in total.
12.98 sec, 1.60 sec, 2922.80 MB/s
-----------------------------------------------------------------------------

[9/176]: Loading consolidated.00/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
22373138432 params in total.
44746276864 bytes in total.
14.63 sec, 1.65 sec, 2917.03 MB/s
-----------------------------------------------------------------------------

[10/176]: Loading consolidated.00/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
24843976704 params in total.
49687953408 bytes in total.
16.31 sec, 1.68 sec, 2904.91 MB/s
-----------------------------------------------------------------------------

[11/176]: Loading consolidated.00/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
25633964032 params in total.
51267928064 bytes in total.
16.88 sec, 0.57 sec, 2895.95 MB/s
-----------------------------------------------------------------------------

[12/176]: Loading consolidated.01/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
28127051776 params in total.
56254103552 bytes in total.
18.39 sec, 1.50 sec, 2918.03 MB/s
-----------------------------------------------------------------------------

[13/176]: Loading consolidated.01/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
30618894336 params in total.
61237788672 bytes in total.
19.97 sec, 1.59 sec, 2924.30 MB/s
-----------------------------------------------------------------------------

[14/176]: Loading consolidated.01/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
33106509824 params in total.
66213019648 bytes in total.
21.62 sec, 1.65 sec, 2920.62 MB/s
-----------------------------------------------------------------------------

[15/176]: Loading consolidated.01/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
35577348096 params in total.
71154696192 bytes in total.
23.25 sec, 1.63 sec, 2918.06 MB/s
-----------------------------------------------------------------------------

[16/176]: Loading consolidated.01/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
38069190656 params in total.
76138381312 bytes in total.
24.80 sec, 1.55 sec, 2927.85 MB/s
-----------------------------------------------------------------------------

[17/176]: Loading consolidated.01/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
40556806144 params in total.
81113612288 bytes in total.
26.46 sec, 1.66 sec, 2923.52 MB/s
-----------------------------------------------------------------------------

[18/176]: Loading consolidated.01/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
43027644416 params in total.
86055288832 bytes in total.
28.08 sec, 1.62 sec, 2922.26 MB/s
-----------------------------------------------------------------------------

[19/176]: Loading consolidated.01/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
45519486976 params in total.
91038973952 bytes in total.
29.65 sec, 1.56 sec, 2928.49 MB/s
-----------------------------------------------------------------------------

[20/176]: Loading consolidated.01/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
48007102464 params in total.
96014204928 bytes in total.
31.29 sec, 1.65 sec, 2925.95 MB/s
-----------------------------------------------------------------------------

[21/176]: Loading consolidated.01/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
50477940736 params in total.
100955881472 bytes in total.
32.94 sec, 1.64 sec, 2922.99 MB/s
-----------------------------------------------------------------------------

[22/176]: Loading consolidated.01/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
51267928064 params in total.
102535856128 bytes in total.
33.52 sec, 0.58 sec, 2917.53 MB/s
-----------------------------------------------------------------------------

[23/176]: Loading consolidated.02/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
53761015808 params in total.
107522031616 bytes in total.
35.00 sec, 1.48 sec, 2929.91 MB/s
-----------------------------------------------------------------------------

[24/176]: Loading consolidated.02/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
56252858368 params in total.
112505716736 bytes in total.
36.55 sec, 1.55 sec, 2935.66 MB/s
-----------------------------------------------------------------------------

[25/176]: Loading consolidated.02/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
58740473856 params in total.
117480947712 bytes in total.
38.18 sec, 1.63 sec, 2934.63 MB/s
-----------------------------------------------------------------------------

[26/176]: Loading consolidated.02/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
61211312128 params in total.
122422624256 bytes in total.
39.80 sec, 1.62 sec, 2933.26 MB/s
-----------------------------------------------------------------------------

[27/176]: Loading consolidated.02/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
63703154688 params in total.
127406309376 bytes in total.
41.37 sec, 1.57 sec, 2937.02 MB/s
-----------------------------------------------------------------------------

[28/176]: Loading consolidated.02/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
66190770176 params in total.
132381540352 bytes in total.
43.02 sec, 1.65 sec, 2934.86 MB/s
-----------------------------------------------------------------------------

[29/176]: Loading consolidated.02/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
68661608448 params in total.
137323216896 bytes in total.
44.67 sec, 1.65 sec, 2932.08 MB/s
-----------------------------------------------------------------------------

[30/176]: Loading consolidated.02/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
71153451008 params in total.
142306902016 bytes in total.
46.22 sec, 1.55 sec, 2936.36 MB/s
-----------------------------------------------------------------------------

[31/176]: Loading consolidated.02/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
73641066496 params in total.
147282132992 bytes in total.
47.89 sec, 1.67 sec, 2932.79 MB/s
-----------------------------------------------------------------------------

[32/176]: Loading consolidated.02/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
76111904768 params in total.
152223809536 bytes in total.
49.54 sec, 1.64 sec, 2930.66 MB/s
-----------------------------------------------------------------------------

[33/176]: Loading consolidated.02/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
76901892096 params in total.
153803784192 bytes in total.
50.13 sec, 0.60 sec, 2925.69 MB/s
-----------------------------------------------------------------------------

[34/176]: Loading consolidated.03/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
79394979840 params in total.
158789959680 bytes in total.
52.67 sec, 2.54 sec, 2875.05 MB/s
-----------------------------------------------------------------------------

[35/176]: Loading consolidated.03/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
81886822400 params in total.
163773644800 bytes in total.
55.86 sec, 3.19 sec, 2795.86 MB/s
-----------------------------------------------------------------------------

[36/176]: Loading consolidated.03/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
84374437888 params in total.
168748875776 bytes in total.
58.62 sec, 2.76 sec, 2745.30 MB/s
-----------------------------------------------------------------------------

[37/176]: Loading consolidated.03/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
86845276160 params in total.
173690552320 bytes in total.
61.36 sec, 2.74 sec, 2699.41 MB/s
-----------------------------------------------------------------------------

[38/176]: Loading consolidated.03/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
89337118720 params in total.
178674237440 bytes in total.
64.11 sec, 2.75 sec, 2657.95 MB/s
-----------------------------------------------------------------------------

[39/176]: Loading consolidated.03/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
91824734208 params in total.
183649468416 bytes in total.
69.34 sec, 5.23 sec, 2525.79 MB/s
-----------------------------------------------------------------------------

[40/176]: Loading consolidated.03/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
94295572480 params in total.
188591144960 bytes in total.
72.10 sec, 2.76 sec, 2494.60 MB/s
-----------------------------------------------------------------------------

[41/176]: Loading consolidated.03/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
96787415040 params in total.
193574830080 bytes in total.
74.84 sec, 2.74 sec, 2466.83 MB/s
-----------------------------------------------------------------------------

[42/176]: Loading consolidated.03/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
99275030528 params in total.
198550061056 bytes in total.
77.62 sec, 2.79 sec, 2439.37 MB/s
-----------------------------------------------------------------------------

[43/176]: Loading consolidated.03/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
101745868800 params in total.
203491737600 bytes in total.
80.58 sec, 2.95 sec, 2408.43 MB/s
-----------------------------------------------------------------------------

[44/176]: Loading consolidated.03/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
102535856128 params in total.
205071712256 bytes in total.
81.57 sec, 0.99 sec, 2397.61 MB/s
-----------------------------------------------------------------------------

[45/176]: Loading consolidated.04/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
105028943872 params in total.
210057887744 bytes in total.
84.21 sec, 2.64 sec, 2378.78 MB/s
-----------------------------------------------------------------------------

[46/176]: Loading consolidated.04/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
107520786432 params in total.
215041572864 bytes in total.
86.98 sec, 2.76 sec, 2357.89 MB/s
-----------------------------------------------------------------------------

[47/176]: Loading consolidated.04/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
110008401920 params in total.
220016803840 bytes in total.
89.74 sec, 2.77 sec, 2338.04 MB/s
-----------------------------------------------------------------------------

[48/176]: Loading consolidated.04/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
112479240192 params in total.
224958480384 bytes in total.
92.53 sec, 2.79 sec, 2318.49 MB/s
-----------------------------------------------------------------------------

[49/176]: Loading consolidated.04/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
114971082752 params in total.
229942165504 bytes in total.
95.30 sec, 2.77 sec, 2301.05 MB/s
-----------------------------------------------------------------------------

[50/176]: Loading consolidated.04/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
117458698240 params in total.
234917396480 bytes in total.
98.26 sec, 2.96 sec, 2279.94 MB/s
-----------------------------------------------------------------------------

[51/176]: Loading consolidated.04/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
119929536512 params in total.
239859073024 bytes in total.
101.08 sec, 2.81 sec, 2263.13 MB/s
-----------------------------------------------------------------------------

[52/176]: Loading consolidated.04/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
122421379072 params in total.
244842758144 bytes in total.
103.85 sec, 2.78 sec, 2248.33 MB/s
-----------------------------------------------------------------------------

[53/176]: Loading consolidated.04/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
124908994560 params in total.
249817989120 bytes in total.
106.66 sec, 2.81 sec, 2233.65 MB/s
-----------------------------------------------------------------------------

[54/176]: Loading consolidated.04/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
127379832832 params in total.
254759665664 bytes in total.
109.44 sec, 2.78 sec, 2219.95 MB/s
-----------------------------------------------------------------------------

[55/176]: Loading consolidated.04/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
128169820160 params in total.
256339640320 bytes in total.
110.43 sec, 0.99 sec, 2213.66 MB/s
-----------------------------------------------------------------------------

[56/176]: Loading consolidated.05/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
130662907904 params in total.
261325815808 bytes in total.
113.06 sec, 2.62 sec, 2204.39 MB/s
-----------------------------------------------------------------------------

[57/176]: Loading consolidated.05/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
133154750464 params in total.
266309500928 bytes in total.
116.28 sec, 3.23 sec, 2184.12 MB/s
-----------------------------------------------------------------------------

[58/176]: Loading consolidated.05/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
135642365952 params in total.
271284731904 bytes in total.
119.05 sec, 2.77 sec, 2173.24 MB/s
-----------------------------------------------------------------------------

[59/176]: Loading consolidated.05/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
138113204224 params in total.
276226408448 bytes in total.
121.79 sec, 2.74 sec, 2163.03 MB/s
-----------------------------------------------------------------------------

[60/176]: Loading consolidated.05/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
140605046784 params in total.
281210093568 bytes in total.
124.49 sec, 2.71 sec, 2154.17 MB/s
-----------------------------------------------------------------------------

[61/176]: Loading consolidated.05/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
143092662272 params in total.
286185324544 bytes in total.
127.24 sec, 2.74 sec, 2144.99 MB/s
-----------------------------------------------------------------------------

[62/176]: Loading consolidated.05/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
145563500544 params in total.
291127001088 bytes in total.
130.29 sec, 3.05 sec, 2131.00 MB/s
-----------------------------------------------------------------------------

[63/176]: Loading consolidated.05/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
148055343104 params in total.
296110686208 bytes in total.
133.00 sec, 2.72 sec, 2123.21 MB/s
-----------------------------------------------------------------------------

[64/176]: Loading consolidated.05/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
150542958592 params in total.
301085917184 bytes in total.
135.78 sec, 2.78 sec, 2114.65 MB/s
-----------------------------------------------------------------------------

[65/176]: Loading consolidated.05/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
153013796864 params in total.
306027593728 bytes in total.
138.53 sec, 2.74 sec, 2106.82 MB/s
-----------------------------------------------------------------------------

[66/176]: Loading consolidated.05/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
153803784192 params in total.
307607568384 bytes in total.
139.50 sec, 0.97 sec, 2102.96 MB/s
-----------------------------------------------------------------------------

[67/176]: Loading consolidated.06/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
156296871936 params in total.
312593743872 bytes in total.
142.11 sec, 2.61 sec, 2097.80 MB/s
-----------------------------------------------------------------------------

[68/176]: Loading consolidated.06/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
158788714496 params in total.
317577428992 bytes in total.
145.32 sec, 3.21 sec, 2084.13 MB/s
-----------------------------------------------------------------------------

[69/176]: Loading consolidated.06/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
161276329984 params in total.
322552659968 bytes in total.
148.08 sec, 2.76 sec, 2077.27 MB/s
-----------------------------------------------------------------------------

[70/176]: Loading consolidated.06/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
163747168256 params in total.
327494336512 bytes in total.
150.83 sec, 2.75 sec, 2070.70 MB/s
-----------------------------------------------------------------------------

[71/176]: Loading consolidated.06/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
166239010816 params in total.
332478021632 bytes in total.
153.56 sec, 2.73 sec, 2064.83 MB/s
-----------------------------------------------------------------------------

[72/176]: Loading consolidated.06/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
168726626304 params in total.
337453252608 bytes in total.
156.34 sec, 2.78 sec, 2058.43 MB/s
-----------------------------------------------------------------------------

[73/176]: Loading consolidated.06/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
171197464576 params in total.
342394929152 bytes in total.
159.15 sec, 2.80 sec, 2051.79 MB/s
-----------------------------------------------------------------------------

[74/176]: Loading consolidated.06/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
173689307136 params in total.
347378614272 bytes in total.
162.45 sec, 3.30 sec, 2039.34 MB/s
-----------------------------------------------------------------------------

[75/176]: Loading consolidated.06/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
176176922624 params in total.
352353845248 bytes in total.
165.26 sec, 2.81 sec, 2033.34 MB/s
-----------------------------------------------------------------------------

[76/176]: Loading consolidated.06/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
178647760896 params in total.
357295521792 bytes in total.
168.05 sec, 2.79 sec, 2027.64 MB/s
-----------------------------------------------------------------------------

[77/176]: Loading consolidated.06/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
179437748224 params in total.
358875496448 bytes in total.
169.06 sec, 1.01 sec, 2024.47 MB/s
-----------------------------------------------------------------------------

[78/176]: Loading consolidated.07/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
181930835968 params in total.
363861671936 bytes in total.
171.69 sec, 2.64 sec, 2021.06 MB/s
-----------------------------------------------------------------------------

[79/176]: Loading consolidated.07/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
184422678528 params in total.
368845357056 bytes in total.
174.84 sec, 3.14 sec, 2011.92 MB/s
-----------------------------------------------------------------------------

[80/176]: Loading consolidated.07/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
186910294016 params in total.
373820588032 bytes in total.
177.64 sec, 2.80 sec, 2006.88 MB/s
-----------------------------------------------------------------------------

[81/176]: Loading consolidated.07/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
189381132288 params in total.
378762264576 bytes in total.
180.42 sec, 2.78 sec, 2002.12 MB/s
-----------------------------------------------------------------------------

[82/176]: Loading consolidated.07/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
191872974848 params in total.
383745949696 bytes in total.
183.15 sec, 2.74 sec, 1998.15 MB/s
-----------------------------------------------------------------------------

[83/176]: Loading consolidated.07/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
194360590336 params in total.
388721180672 bytes in total.
185.94 sec, 2.79 sec, 1993.69 MB/s
-----------------------------------------------------------------------------

[84/176]: Loading consolidated.07/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
196831428608 params in total.
393662857216 bytes in total.
188.70 sec, 2.76 sec, 1989.50 MB/s
-----------------------------------------------------------------------------

[85/176]: Loading consolidated.07/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
199323271168 params in total.
398646542336 bytes in total.
191.79 sec, 3.09 sec, 1982.25 MB/s
-----------------------------------------------------------------------------

[86/176]: Loading consolidated.07/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
201810886656 params in total.
403621773312 bytes in total.
194.69 sec, 2.90 sec, 1977.10 MB/s
-----------------------------------------------------------------------------

[87/176]: Loading consolidated.07/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
204281724928 params in total.
408563449856 bytes in total.
197.50 sec, 2.81 sec, 1972.88 MB/s
-----------------------------------------------------------------------------

[88/176]: Loading consolidated.07/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
205071712256 params in total.
410143424512 bytes in total.
198.50 sec, 1.00 sec, 1970.51 MB/s
-----------------------------------------------------------------------------

[89/176]: Loading consolidated.08/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
207564800000 params in total.
415129600000 bytes in total.
201.15 sec, 2.65 sec, 1968.18 MB/s
-----------------------------------------------------------------------------

[90/176]: Loading consolidated.08/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
210056642560 params in total.
420113285120 bytes in total.
204.33 sec, 3.18 sec, 1960.77 MB/s
-----------------------------------------------------------------------------

[91/176]: Loading consolidated.08/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
212544258048 params in total.
425088516096 bytes in total.
207.17 sec, 2.83 sec, 1956.86 MB/s
-----------------------------------------------------------------------------

[92/176]: Loading consolidated.08/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
215015096320 params in total.
430030192640 bytes in total.
209.98 sec, 2.81 sec, 1953.13 MB/s
-----------------------------------------------------------------------------

[93/176]: Loading consolidated.08/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
217506938880 params in total.
435013877760 bytes in total.
212.75 sec, 2.78 sec, 1949.96 MB/s
-----------------------------------------------------------------------------

[94/176]: Loading consolidated.08/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
219994554368 params in total.
439989108736 bytes in total.
215.55 sec, 2.80 sec, 1946.68 MB/s
-----------------------------------------------------------------------------

[95/176]: Loading consolidated.08/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
222465392640 params in total.
444930785280 bytes in total.
218.32 sec, 2.77 sec, 1943.55 MB/s
-----------------------------------------------------------------------------

[96/176]: Loading consolidated.08/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
224957235200 params in total.
449914470400 bytes in total.
221.08 sec, 2.75 sec, 1940.84 MB/s
-----------------------------------------------------------------------------

[97/176]: Loading consolidated.08/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
227444850688 params in total.
454889701376 bytes in total.
224.37 sec, 3.29 sec, 1933.52 MB/s
-----------------------------------------------------------------------------

[98/176]: Loading consolidated.08/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
229915688960 params in total.
459831377920 bytes in total.
227.72 sec, 3.36 sec, 1925.71 MB/s
-----------------------------------------------------------------------------

[99/176]: Loading consolidated.08/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
230705676288 params in total.
461411352576 bytes in total.
228.71 sec, 0.99 sec, 1923.99 MB/s
-----------------------------------------------------------------------------

[100/176]: Loading consolidated.09/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
233198764032 params in total.
466397528064 bytes in total.
231.32 sec, 2.61 sec, 1922.82 MB/s
-----------------------------------------------------------------------------

[101/176]: Loading consolidated.09/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
235690606592 params in total.
471381213184 bytes in total.
234.06 sec, 2.74 sec, 1920.64 MB/s
-----------------------------------------------------------------------------

[102/176]: Loading consolidated.09/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
238178222080 params in total.
476356444160 bytes in total.
236.82 sec, 2.76 sec, 1918.29 MB/s
-----------------------------------------------------------------------------

[103/176]: Loading consolidated.09/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
240649060352 params in total.
481298120704 bytes in total.
239.57 sec, 2.75 sec, 1915.96 MB/s
-----------------------------------------------------------------------------

[104/176]: Loading consolidated.09/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
243140902912 params in total.
486281805824 bytes in total.
242.29 sec, 2.72 sec, 1914.08 MB/s
-----------------------------------------------------------------------------

[105/176]: Loading consolidated.09/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
245628518400 params in total.
491257036800 bytes in total.
245.05 sec, 2.77 sec, 1911.84 MB/s
-----------------------------------------------------------------------------

[106/176]: Loading consolidated.09/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
248099356672 params in total.
496198713344 bytes in total.
247.80 sec, 2.75 sec, 1909.68 MB/s
-----------------------------------------------------------------------------

[107/176]: Loading consolidated.09/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
250591199232 params in total.
501182398464 bytes in total.
250.52 sec, 2.73 sec, 1907.85 MB/s
-----------------------------------------------------------------------------

[108/176]: Loading consolidated.09/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
253078814720 params in total.
506157629440 bytes in total.
253.44 sec, 2.92 sec, 1904.62 MB/s
-----------------------------------------------------------------------------

[109/176]: Loading consolidated.09/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
255549652992 params in total.
511099305984 bytes in total.
256.68 sec, 3.24 sec, 1898.96 MB/s
-----------------------------------------------------------------------------

[110/176]: Loading consolidated.09/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
256339640320 params in total.
512679280640 bytes in total.
257.67 sec, 0.99 sec, 1897.49 MB/s
-----------------------------------------------------------------------------

[111/176]: Loading consolidated.10/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
258832728064 params in total.
517665456128 bytes in total.
260.31 sec, 2.64 sec, 1896.54 MB/s
-----------------------------------------------------------------------------

[112/176]: Loading consolidated.10/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
261324570624 params in total.
522649141248 bytes in total.
263.05 sec, 2.74 sec, 1894.81 MB/s
-----------------------------------------------------------------------------

[113/176]: Loading consolidated.10/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
263812186112 params in total.
527624372224 bytes in total.
265.85 sec, 2.80 sec, 1892.71 MB/s
-----------------------------------------------------------------------------

[114/176]: Loading consolidated.10/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
266283024384 params in total.
532566048768 bytes in total.
268.64 sec, 2.79 sec, 1890.61 MB/s
-----------------------------------------------------------------------------

[115/176]: Loading consolidated.10/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
268774866944 params in total.
537549733888 bytes in total.
271.42 sec, 2.78 sec, 1888.77 MB/s
-----------------------------------------------------------------------------

[116/176]: Loading consolidated.10/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
271262482432 params in total.
542524964864 bytes in total.
274.24 sec, 2.82 sec, 1886.64 MB/s
-----------------------------------------------------------------------------

[117/176]: Loading consolidated.10/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
273733320704 params in total.
547466641408 bytes in total.
277.05 sec, 2.81 sec, 1884.50 MB/s
-----------------------------------------------------------------------------

[118/176]: Loading consolidated.10/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
276225163264 params in total.
552450326528 bytes in total.
279.84 sec, 2.79 sec, 1882.68 MB/s
-----------------------------------------------------------------------------

[119/176]: Loading consolidated.10/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
278712778752 params in total.
557425557504 bytes in total.
282.68 sec, 2.84 sec, 1880.56 MB/s
-----------------------------------------------------------------------------

[120/176]: Loading consolidated.10/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
281183617024 params in total.
562367234048 bytes in total.
285.95 sec, 3.27 sec, 1875.53 MB/s
-----------------------------------------------------------------------------

[121/176]: Loading consolidated.10/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
281973604352 params in total.
563947208704 bytes in total.
286.96 sec, 1.00 sec, 1874.23 MB/s
-----------------------------------------------------------------------------

[122/176]: Loading consolidated.11/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
284466692096 params in total.
568933384192 bytes in total.
289.69 sec, 2.74 sec, 1872.94 MB/s
-----------------------------------------------------------------------------

[123/176]: Loading consolidated.11/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
286958534656 params in total.
573917069312 bytes in total.
292.78 sec, 3.08 sec, 1869.44 MB/s
-----------------------------------------------------------------------------

[124/176]: Loading consolidated.11/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
289446150144 params in total.
578892300288 bytes in total.
295.58 sec, 2.80 sec, 1867.79 MB/s
-----------------------------------------------------------------------------

[125/176]: Loading consolidated.11/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
291916988416 params in total.
583833976832 bytes in total.
298.38 sec, 2.80 sec, 1866.03 MB/s
-----------------------------------------------------------------------------

[126/176]: Loading consolidated.11/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
294408830976 params in total.
588817661952 bytes in total.
301.16 sec, 2.78 sec, 1864.58 MB/s
-----------------------------------------------------------------------------

[127/176]: Loading consolidated.11/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
296896446464 params in total.
593792892928 bytes in total.
303.97 sec, 2.81 sec, 1862.98 MB/s
-----------------------------------------------------------------------------

[128/176]: Loading consolidated.11/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
299367284736 params in total.
598734569472 bytes in total.
306.74 sec, 2.77 sec, 1861.51 MB/s
-----------------------------------------------------------------------------

[129/176]: Loading consolidated.11/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
301859127296 params in total.
603718254592 bytes in total.
309.50 sec, 2.76 sec, 1860.29 MB/s
-----------------------------------------------------------------------------

[130/176]: Loading consolidated.11/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
304346742784 params in total.
608693485568 bytes in total.
312.29 sec, 2.79 sec, 1858.84 MB/s
-----------------------------------------------------------------------------

[131/176]: Loading consolidated.11/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
306817581056 params in total.
613635162112 bytes in total.
315.30 sec, 3.01 sec, 1856.05 MB/s
-----------------------------------------------------------------------------

[132/176]: Loading consolidated.11/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
307607568384 params in total.
615215136768 bytes in total.
316.29 sec, 0.99 sec, 1855.00 MB/s
-----------------------------------------------------------------------------

[133/176]: Loading consolidated.12/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
310100656128 params in total.
620201312256 bytes in total.
318.91 sec, 2.62 sec, 1854.69 MB/s
-----------------------------------------------------------------------------

[134/176]: Loading consolidated.12/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
312592498688 params in total.
625184997376 bytes in total.
322.12 sec, 3.21 sec, 1850.94 MB/s
-----------------------------------------------------------------------------

[135/176]: Loading consolidated.12/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
315080114176 params in total.
630160228352 bytes in total.
324.93 sec, 2.81 sec, 1849.52 MB/s
-----------------------------------------------------------------------------

[136/176]: Loading consolidated.12/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
317550952448 params in total.
635101904896 bytes in total.
327.72 sec, 2.78 sec, 1848.18 MB/s
-----------------------------------------------------------------------------

[137/176]: Loading consolidated.12/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
320042795008 params in total.
640085590016 bytes in total.
330.46 sec, 2.74 sec, 1847.22 MB/s
-----------------------------------------------------------------------------

[138/176]: Loading consolidated.12/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
322530410496 params in total.
645060820992 bytes in total.
333.26 sec, 2.80 sec, 1845.96 MB/s
-----------------------------------------------------------------------------

[139/176]: Loading consolidated.12/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
325001248768 params in total.
650002497536 bytes in total.
336.01 sec, 2.75 sec, 1844.87 MB/s
-----------------------------------------------------------------------------

[140/176]: Loading consolidated.12/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
327493091328 params in total.
654986182656 bytes in total.
338.77 sec, 2.76 sec, 1843.88 MB/s
-----------------------------------------------------------------------------

[141/176]: Loading consolidated.12/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
329980706816 params in total.
659961413632 bytes in total.
341.57 sec, 2.80 sec, 1842.65 MB/s
-----------------------------------------------------------------------------

[142/176]: Loading consolidated.12/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
332451545088 params in total.
664903090176 bytes in total.
344.33 sec, 2.76 sec, 1841.56 MB/s
-----------------------------------------------------------------------------

[143/176]: Loading consolidated.12/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
333241532416 params in total.
666483064832 bytes in total.
345.31 sec, 0.98 sec, 1840.68 MB/s
-----------------------------------------------------------------------------

[144/176]: Loading consolidated.13/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
335734620160 params in total.
671469240320 bytes in total.
348.40 sec, 3.09 sec, 1838.01 MB/s
-----------------------------------------------------------------------------

[145/176]: Loading consolidated.13/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
338226462720 params in total.
676452925440 bytes in total.
351.63 sec, 3.23 sec, 1834.63 MB/s
-----------------------------------------------------------------------------

[146/176]: Loading consolidated.13/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
340714078208 params in total.
681428156416 bytes in total.
354.40 sec, 2.77 sec, 1833.70 MB/s
-----------------------------------------------------------------------------

[147/176]: Loading consolidated.13/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
343184916480 params in total.
686369832960 bytes in total.
357.16 sec, 2.76 sec, 1832.72 MB/s
-----------------------------------------------------------------------------

[148/176]: Loading consolidated.13/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
345676759040 params in total.
691353518080 bytes in total.
359.89 sec, 2.73 sec, 1832.03 MB/s
-----------------------------------------------------------------------------

[149/176]: Loading consolidated.13/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
348164374528 params in total.
696328749056 bytes in total.
362.67 sec, 2.78 sec, 1831.05 MB/s
-----------------------------------------------------------------------------

[150/176]: Loading consolidated.13/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
350635212800 params in total.
701270425600 bytes in total.
365.42 sec, 2.75 sec, 1830.17 MB/s
-----------------------------------------------------------------------------

[151/176]: Loading consolidated.13/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
353127055360 params in total.
706254110720 bytes in total.
368.16 sec, 2.74 sec, 1829.45 MB/s
-----------------------------------------------------------------------------

[152/176]: Loading consolidated.13/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
355614670848 params in total.
711229341696 bytes in total.
370.95 sec, 2.79 sec, 1828.50 MB/s
-----------------------------------------------------------------------------

[153/176]: Loading consolidated.13/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
358085509120 params in total.
716171018240 bytes in total.
373.73 sec, 2.78 sec, 1827.51 MB/s
-----------------------------------------------------------------------------

[154/176]: Loading consolidated.13/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
358875496448 params in total.
717750992896 bytes in total.
374.72 sec, 0.99 sec, 1826.70 MB/s
-----------------------------------------------------------------------------

[155/176]: Loading consolidated.14/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
361368584192 params in total.
722737168384 bytes in total.
377.62 sec, 2.90 sec, 1825.26 MB/s
-----------------------------------------------------------------------------

[156/176]: Loading consolidated.14/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
363860426752 params in total.
727720853504 bytes in total.
380.99 sec, 3.36 sec, 1821.61 MB/s
-----------------------------------------------------------------------------

[157/176]: Loading consolidated.14/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
366348042240 params in total.
732696084480 bytes in total.
383.80 sec, 2.81 sec, 1820.63 MB/s
-----------------------------------------------------------------------------

[158/176]: Loading consolidated.14/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
368818880512 params in total.
737637761024 bytes in total.
386.59 sec, 2.79 sec, 1819.66 MB/s
-----------------------------------------------------------------------------

[159/176]: Loading consolidated.14/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
371310723072 params in total.
742621446144 bytes in total.
389.36 sec, 2.76 sec, 1818.95 MB/s
-----------------------------------------------------------------------------

[160/176]: Loading consolidated.14/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
373798338560 params in total.
747596677120 bytes in total.
392.16 sec, 2.81 sec, 1818.04 MB/s
-----------------------------------------------------------------------------

[161/176]: Loading consolidated.14/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
376269176832 params in total.
752538353664 bytes in total.
394.96 sec, 2.80 sec, 1817.07 MB/s
-----------------------------------------------------------------------------

[162/176]: Loading consolidated.14/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
378761019392 params in total.
757522038784 bytes in total.
397.76 sec, 2.79 sec, 1816.26 MB/s
-----------------------------------------------------------------------------

[163/176]: Loading consolidated.14/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
381248634880 params in total.
762497269760 bytes in total.
400.56 sec, 2.80 sec, 1815.42 MB/s
-----------------------------------------------------------------------------

[164/176]: Loading consolidated.14/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
383719473152 params in total.
767438946304 bytes in total.
403.32 sec, 2.76 sec, 1814.66 MB/s
-----------------------------------------------------------------------------

[165/176]: Loading consolidated.14/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
384509460480 params in total.
769018920960 bytes in total.
404.31 sec, 0.99 sec, 1813.93 MB/s
-----------------------------------------------------------------------------

[166/176]: Loading consolidated.15/consolidated-00001-of-00011.pth
   0 :  131334144 : tok_embeddings.weight               : [8016, 16384]   : torch.bfloat16
   1 :   16777216 : layers.0.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   16777216 : layers.0.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
   5 :   54525952 : layers.0.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
   6 :   54525952 : layers.0.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
   7 :   54525952 : layers.0.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   16777216 : layers.1.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   16777216 : layers.1.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  14 :   54525952 : layers.1.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  15 :   54525952 : layers.1.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  16 :   54525952 : layers.1.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   16777216 : layers.2.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   16777216 : layers.2.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  23 :   54525952 : layers.2.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  24 :   54525952 : layers.2.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  25 :   54525952 : layers.2.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   16777216 : layers.3.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   16777216 : layers.3.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  32 :   54525952 : layers.3.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  33 :   54525952 : layers.3.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  34 :   54525952 : layers.3.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   16777216 : layers.4.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   16777216 : layers.4.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  41 :   54525952 : layers.4.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  42 :   54525952 : layers.4.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  43 :   54525952 : layers.4.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   16777216 : layers.5.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   16777216 : layers.5.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  50 :   54525952 : layers.5.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  51 :   54525952 : layers.5.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  52 :   54525952 : layers.5.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  53 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
  54 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
  55 :   16777216 : layers.6.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  56 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  57 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  58 :   16777216 : layers.6.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  59 :   54525952 : layers.6.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  60 :   54525952 : layers.6.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  61 :   54525952 : layers.6.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  62 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  63 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  64 :   16777216 : layers.7.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  65 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  66 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  67 :   16777216 : layers.7.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  68 :   54525952 : layers.7.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  69 :   54525952 : layers.7.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  70 :   54525952 : layers.7.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  71 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  72 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  73 :   16777216 : layers.8.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  74 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  75 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  76 :   16777216 : layers.8.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  77 :   54525952 : layers.8.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  78 :   54525952 : layers.8.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  79 :   54525952 : layers.8.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  80 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  81 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  82 :   16777216 : layers.9.attention.wq.weight        : [1024, 16384]   : torch.bfloat16
  83 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  84 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  85 :   16777216 : layers.9.attention.wo.weight        : [16384, 1024]   : torch.bfloat16
  86 :   54525952 : layers.9.feed_forward.w1.weight     : [3328, 16384]   : torch.bfloat16
  87 :   54525952 : layers.9.feed_forward.w3.weight     : [3328, 16384]   : torch.bfloat16
  88 :   54525952 : layers.9.feed_forward.w2.weight     : [16384, 3328]   : torch.bfloat16
  89 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  90 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  91 :   16777216 : layers.10.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  92 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  93 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  94 :   16777216 : layers.10.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  95 :   54525952 : layers.10.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  96 :   54525952 : layers.10.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  97 :   54525952 : layers.10.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  98 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  99 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
 100 :   16777216 : layers.11.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 101 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 102 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 103 :   16777216 : layers.11.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 104 :   54525952 : layers.11.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 105 :   54525952 : layers.11.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
387002548224 params in total.
774005096448 bytes in total.
406.93 sec, 2.61 sec, 1813.96 MB/s
-----------------------------------------------------------------------------

[167/176]: Loading consolidated.15/consolidated-00002-of-00011.pth
   0 :   54525952 : layers.11.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.12.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.12.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.12.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.12.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.12.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.13.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.13.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.13.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.13.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.13.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.14.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.14.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.14.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.14.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.14.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.15.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.15.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.15.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.15.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.15.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.16.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.16.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.16.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.16.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.16.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.17.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.17.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.17.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.17.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.17.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.18.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.18.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.18.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.18.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.18.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.19.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.19.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.19.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.19.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.19.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.20.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.20.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.20.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.20.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.20.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.21.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.21.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.21.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.21.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.21.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.22.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.22.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.22.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.22.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.22.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.23.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.23.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.23.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.23.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.23.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.24.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
389494390784 params in total.
778988781568 bytes in total.
410.19 sec, 3.27 sec, 1811.10 MB/s
-----------------------------------------------------------------------------

[168/176]: Loading consolidated.15/consolidated-00003-of-00011.pth
   0 :   16777216 : layers.24.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.24.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.24.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.24.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.25.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.25.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.25.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.25.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.25.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.26.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.26.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.26.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.26.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.26.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.27.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.27.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.27.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.27.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.27.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.28.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.28.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.28.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.28.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.28.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.29.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.29.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.29.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.29.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.29.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.30.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.30.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.30.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.30.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.30.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.31.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.31.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.31.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.31.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.31.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.32.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.32.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.32.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.32.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.32.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.33.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.33.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.33.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.33.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.33.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.34.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.34.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.34.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.34.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.34.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.35.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.35.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.35.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.35.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.35.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.36.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.36.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.36.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
391982006272 params in total.
783964012544 bytes in total.
412.99 sec, 2.80 sec, 1810.31 MB/s
-----------------------------------------------------------------------------

[169/176]: Loading consolidated.15/consolidated-00004-of-00011.pth
   0 :   54525952 : layers.36.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.36.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.37.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.37.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.37.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.37.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.37.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.38.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.38.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.38.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.38.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.38.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.39.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.39.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.39.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.39.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.39.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.40.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.40.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.40.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.40.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.40.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.41.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.41.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.41.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.41.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.41.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.42.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.42.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.42.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.42.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.42.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.43.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.43.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.43.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.43.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.43.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.44.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.44.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.44.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.44.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.44.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.45.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.45.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.45.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.45.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.45.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.46.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.46.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.46.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.46.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.46.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.47.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.47.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.47.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.47.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.47.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.48.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.48.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.48.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.48.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
394452844544 params in total.
788905689088 bytes in total.
415.78 sec, 2.79 sec, 1809.51 MB/s
-----------------------------------------------------------------------------

[170/176]: Loading consolidated.15/consolidated-00005-of-00011.pth
   0 :   54525952 : layers.48.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.49.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.49.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.49.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.49.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.49.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.50.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.50.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.50.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.50.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.50.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.51.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.51.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.51.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.51.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.51.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.52.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.52.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.52.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.52.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.52.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.53.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.53.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.53.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.53.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.53.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.54.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.54.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.54.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.54.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.54.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.55.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.55.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.55.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.55.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.55.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.56.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.56.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.56.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.56.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.56.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.57.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.57.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.57.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.57.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.57.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.58.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.58.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.58.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.58.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.58.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.59.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.59.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.59.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.59.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.59.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.60.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.60.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.60.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.60.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.60.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.61.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
396944687104 params in total.
793889374208 bytes in total.
418.54 sec, 2.76 sec, 1808.95 MB/s
-----------------------------------------------------------------------------

[171/176]: Loading consolidated.15/consolidated-00006-of-00011.pth
   0 :   16777216 : layers.61.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.61.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.61.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.61.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.62.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.62.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.62.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.62.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.62.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.63.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.63.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.63.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.63.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.63.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  23 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  24 :   16777216 : layers.64.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.64.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.64.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.64.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.64.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  32 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  33 :   16777216 : layers.65.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.65.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.65.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.65.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.65.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
  41 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
  42 :   16777216 : layers.66.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.66.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.66.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.66.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.66.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  50 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  51 :   16777216 : layers.67.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.67.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.67.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.67.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.67.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  59 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  60 :   16777216 : layers.68.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.68.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.68.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.68.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.68.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  68 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  69 :   16777216 : layers.69.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.69.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.69.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.69.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.69.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  77 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  78 :   16777216 : layers.70.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.70.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.70.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.70.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.70.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  86 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  87 :   16777216 : layers.71.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.71.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.71.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.71.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.71.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
  95 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
  96 :   16777216 : layers.72.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.72.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.72.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.72.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.72.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
 104 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
 105 :   16777216 : layers.73.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.73.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.73.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
399432302592 params in total.
798864605184 bytes in total.
421.35 sec, 2.81 sec, 1808.15 MB/s
-----------------------------------------------------------------------------

[172/176]: Loading consolidated.15/consolidated-00007-of-00011.pth
   0 :   54525952 : layers.73.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.73.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   16777216 : layers.74.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.74.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.74.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.74.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.74.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   16777216 : layers.75.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.75.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.75.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.75.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.75.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   16777216 : layers.76.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.76.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.76.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.76.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.76.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   16777216 : layers.77.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.77.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.77.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.77.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.77.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   16777216 : layers.78.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.78.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.78.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.78.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.78.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   16777216 : layers.79.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.79.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.79.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.79.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.79.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  57 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  58 :   16777216 : layers.80.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.80.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.80.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.80.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.80.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  66 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  67 :   16777216 : layers.81.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.81.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.81.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.81.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.81.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  75 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  76 :   16777216 : layers.82.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.82.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.82.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.82.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.82.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  84 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  85 :   16777216 : layers.83.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.83.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.83.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.83.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.83.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
  93 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
  94 :   16777216 : layers.84.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.84.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.84.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.84.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.84.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
 102 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
 103 :   16777216 : layers.85.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.85.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.85.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.85.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
401903140864 params in total.
803806281728 bytes in total.
424.12 sec, 2.77 sec, 1807.45 MB/s
-----------------------------------------------------------------------------

[173/176]: Loading consolidated.15/consolidated-00008-of-00011.pth
   0 :   54525952 : layers.85.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
   2 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
   3 :   16777216 : layers.86.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.86.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.86.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.86.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.86.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  11 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  12 :   16777216 : layers.87.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.87.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.87.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.87.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.87.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  20 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  21 :   16777216 : layers.88.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.88.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.88.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.88.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.88.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  29 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  30 :   16777216 : layers.89.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  31 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  32 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  33 :   16777216 : layers.89.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  34 :   54525952 : layers.89.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  35 :   54525952 : layers.89.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.89.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  37 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
  38 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
  39 :   16777216 : layers.90.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  40 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  41 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  42 :   16777216 : layers.90.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  43 :   54525952 : layers.90.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  44 :   54525952 : layers.90.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.90.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  46 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  47 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  48 :   16777216 : layers.91.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  49 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  50 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  51 :   16777216 : layers.91.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  52 :   54525952 : layers.91.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  53 :   54525952 : layers.91.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.91.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  55 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  56 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  57 :   16777216 : layers.92.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  58 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  59 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  60 :   16777216 : layers.92.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  61 :   54525952 : layers.92.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  62 :   54525952 : layers.92.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.92.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  64 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  65 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  66 :   16777216 : layers.93.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  67 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  68 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  69 :   16777216 : layers.93.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  70 :   54525952 : layers.93.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  71 :   54525952 : layers.93.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.93.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  73 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  74 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  75 :   16777216 : layers.94.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  76 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  77 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  78 :   16777216 : layers.94.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  79 :   54525952 : layers.94.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  80 :   54525952 : layers.94.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.94.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  82 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  83 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  84 :   16777216 : layers.95.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  85 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  86 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  87 :   16777216 : layers.95.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  88 :   54525952 : layers.95.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  89 :   54525952 : layers.95.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.95.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  91 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
  92 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
  93 :   16777216 : layers.96.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
  94 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  95 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  96 :   16777216 : layers.96.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  97 :   54525952 : layers.96.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  98 :   54525952 : layers.96.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.96.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 100 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
 101 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
 102 :   16777216 : layers.97.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 103 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 104 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
 105 :   16777216 : layers.97.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
 106 :   54525952 : layers.97.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
 107 :   54525952 : layers.97.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.97.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
 109 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
 110 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
 111 :   16777216 : layers.98.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
 112 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
 113 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16

-----------------------------------------------------------------------------
404394983424 params in total.
808789966848 bytes in total.
426.89 sec, 2.77 sec, 1806.85 MB/s
-----------------------------------------------------------------------------

[174/176]: Loading consolidated.15/consolidated-00009-of-00011.pth
   0 :   16777216 : layers.98.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
   1 :   54525952 : layers.98.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
   2 :   54525952 : layers.98.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
   3 :   54525952 : layers.98.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
   4 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
   5 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
   6 :   16777216 : layers.99.attention.wq.weight       : [1024, 16384]   : torch.bfloat16
   7 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   8 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   9 :   16777216 : layers.99.attention.wo.weight       : [16384, 1024]   : torch.bfloat16
  10 :   54525952 : layers.99.feed_forward.w1.weight    : [3328, 16384]   : torch.bfloat16
  11 :   54525952 : layers.99.feed_forward.w3.weight    : [3328, 16384]   : torch.bfloat16
  12 :   54525952 : layers.99.feed_forward.w2.weight    : [16384, 3328]   : torch.bfloat16
  13 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  14 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  15 :   16777216 : layers.100.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  16 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  17 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  18 :   16777216 : layers.100.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  19 :   54525952 : layers.100.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  20 :   54525952 : layers.100.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  21 :   54525952 : layers.100.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  22 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  23 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  24 :   16777216 : layers.101.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  25 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  26 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  27 :   16777216 : layers.101.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  28 :   54525952 : layers.101.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  29 :   54525952 : layers.101.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  30 :   54525952 : layers.101.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  31 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
  32 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
  33 :   16777216 : layers.102.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  34 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  35 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  36 :   16777216 : layers.102.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  37 :   54525952 : layers.102.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  38 :   54525952 : layers.102.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  39 :   54525952 : layers.102.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  40 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  41 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  42 :   16777216 : layers.103.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  43 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  44 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  45 :   16777216 : layers.103.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  46 :   54525952 : layers.103.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  47 :   54525952 : layers.103.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  48 :   54525952 : layers.103.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  49 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  50 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  51 :   16777216 : layers.104.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  52 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  53 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  54 :   16777216 : layers.104.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  55 :   54525952 : layers.104.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  56 :   54525952 : layers.104.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  57 :   54525952 : layers.104.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  58 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  59 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  60 :   16777216 : layers.105.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  61 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  62 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  63 :   16777216 : layers.105.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  64 :   54525952 : layers.105.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  65 :   54525952 : layers.105.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  66 :   54525952 : layers.105.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  67 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  68 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  69 :   16777216 : layers.106.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  70 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  71 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  72 :   16777216 : layers.106.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  73 :   54525952 : layers.106.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  74 :   54525952 : layers.106.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  75 :   54525952 : layers.106.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  76 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  77 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  78 :   16777216 : layers.107.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  79 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  80 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  81 :   16777216 : layers.107.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  82 :   54525952 : layers.107.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  83 :   54525952 : layers.107.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  84 :   54525952 : layers.107.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  85 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
  86 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
  87 :   16777216 : layers.108.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  88 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  89 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  90 :   16777216 : layers.108.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  91 :   54525952 : layers.108.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  92 :   54525952 : layers.108.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  93 :   54525952 : layers.108.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  94 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  95 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  96 :   16777216 : layers.109.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  97 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  98 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  99 :   16777216 : layers.109.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 100 :   54525952 : layers.109.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 101 :   54525952 : layers.109.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 102 :   54525952 : layers.109.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 103 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
 104 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
 105 :   16777216 : layers.110.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 106 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 107 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 108 :   16777216 : layers.110.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 109 :   54525952 : layers.110.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
406882598912 params in total.
813765197824 bytes in total.
429.68 sec, 2.79 sec, 1806.17 MB/s
-----------------------------------------------------------------------------

[175/176]: Loading consolidated.15/consolidated-00010-of-00011.pth
   0 :   54525952 : layers.110.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   1 :   54525952 : layers.110.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   2 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   16777216 : layers.111.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   5 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   16777216 : layers.111.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   8 :   54525952 : layers.111.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.111.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  10 :   54525952 : layers.111.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  11 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   16777216 : layers.112.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  14 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   16777216 : layers.112.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  17 :   54525952 : layers.112.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.112.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  19 :   54525952 : layers.112.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  20 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   16777216 : layers.113.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  23 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   16777216 : layers.113.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  26 :   54525952 : layers.113.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.113.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  28 :   54525952 : layers.113.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  29 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   16777216 : layers.114.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  32 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   16777216 : layers.114.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  35 :   54525952 : layers.114.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  36 :   54525952 : layers.114.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  37 :   54525952 : layers.114.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  38 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   16777216 : layers.115.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  41 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   16777216 : layers.115.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  44 :   54525952 : layers.115.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  45 :   54525952 : layers.115.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  46 :   54525952 : layers.115.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  47 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   16777216 : layers.116.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  50 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   16777216 : layers.116.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  53 :   54525952 : layers.116.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  54 :   54525952 : layers.116.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  55 :   54525952 : layers.116.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  56 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  57 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  58 :   16777216 : layers.117.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  59 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  60 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  61 :   16777216 : layers.117.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  62 :   54525952 : layers.117.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  63 :   54525952 : layers.117.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  64 :   54525952 : layers.117.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  65 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  66 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  67 :   16777216 : layers.118.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  68 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  69 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  70 :   16777216 : layers.118.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  71 :   54525952 : layers.118.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  72 :   54525952 : layers.118.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  73 :   54525952 : layers.118.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  74 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  75 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  76 :   16777216 : layers.119.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  77 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  78 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  79 :   16777216 : layers.119.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  80 :   54525952 : layers.119.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  81 :   54525952 : layers.119.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  82 :   54525952 : layers.119.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  83 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
  84 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
  85 :   16777216 : layers.120.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  86 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  87 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  88 :   16777216 : layers.120.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  89 :   54525952 : layers.120.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  90 :   54525952 : layers.120.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  91 :   54525952 : layers.120.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  92 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  93 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  94 :   16777216 : layers.121.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  95 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  96 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  97 :   16777216 : layers.121.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  98 :   54525952 : layers.121.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  99 :   54525952 : layers.121.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
 100 :   54525952 : layers.121.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
 101 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
 102 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
 103 :   16777216 : layers.122.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
 104 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
 105 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
 106 :   16777216 : layers.122.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
 107 :   54525952 : layers.122.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
 108 :   54525952 : layers.122.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
409353437184 params in total.
818706874368 bytes in total.
432.46 sec, 2.78 sec, 1805.44 MB/s
-----------------------------------------------------------------------------

[176/176]: Loading consolidated.15/consolidated-00011-of-00011.pth
   0 :   54525952 : layers.122.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
   1 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
   2 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
   3 :   16777216 : layers.123.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
   4 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   5 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   6 :   16777216 : layers.123.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
   7 :   54525952 : layers.123.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
   8 :   54525952 : layers.123.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
   9 :   54525952 : layers.123.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  10 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  11 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  12 :   16777216 : layers.124.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  13 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  14 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  15 :   16777216 : layers.124.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  16 :   54525952 : layers.124.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  17 :   54525952 : layers.124.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  18 :   54525952 : layers.124.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  19 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  20 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  21 :   16777216 : layers.125.attention.wq.weight      : [1024, 16384]   : torch.bfloat16
  22 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  23 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  24 :   16777216 : layers.125.attention.wo.weight      : [16384, 1024]   : torch.bfloat16
  25 :   54525952 : layers.125.feed_forward.w1.weight   : [3328, 16384]   : torch.bfloat16
  26 :   54525952 : layers.125.feed_forward.w3.weight   : [3328, 16384]   : torch.bfloat16
  27 :   54525952 : layers.125.feed_forward.w2.weight   : [16384, 3328]   : torch.bfloat16
  28 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
  29 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
  30 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
  31 :  131334144 : output.weight                       : [8016, 16384]   : torch.bfloat16
Total number of parameters: 410143424512
410.1434 B
820286849024 Bytes
763.9517 GB
