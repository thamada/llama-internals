{'dim': 16384, 'n_layers': 126, 'n_heads': 128, 'n_kv_heads': 8, 'vocab_size': 128256, 'ffn_dim_multiplier': 1.2, 'multiple_of': 4096, 'norm_eps': 1e-05, 'rope_theta': 500000.0, 'use_scaled_rope': True}
DIM= 16384
000: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00001-of-00022.pth
001: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00002-of-00022.pth
002: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00003-of-00022.pth
003: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00004-of-00022.pth
004: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00005-of-00022.pth
005: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00006-of-00022.pth
006: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00007-of-00022.pth
007: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00008-of-00022.pth
008: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00009-of-00022.pth
009: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00010-of-00022.pth
010: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00011-of-00022.pth
011: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00012-of-00022.pth
012: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00013-of-00022.pth
013: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00014-of-00022.pth
014: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00015-of-00022.pth
015: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00016-of-00022.pth
016: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00017-of-00022.pth
017: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00018-of-00022.pth
018: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00019-of-00022.pth
019: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00020-of-00022.pth
020: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00021-of-00022.pth
021: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00022-of-00022.pth
022: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00001-of-00022.pth
023: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00002-of-00022.pth
024: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00003-of-00022.pth
025: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00004-of-00022.pth
026: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00005-of-00022.pth
027: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00006-of-00022.pth
028: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00007-of-00022.pth
029: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00008-of-00022.pth
030: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00009-of-00022.pth
031: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00010-of-00022.pth
032: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00011-of-00022.pth
033: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00012-of-00022.pth
034: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00013-of-00022.pth
035: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00014-of-00022.pth
036: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00015-of-00022.pth
037: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00016-of-00022.pth
038: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00017-of-00022.pth
039: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00018-of-00022.pth
040: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00019-of-00022.pth
041: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00020-of-00022.pth
042: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00021-of-00022.pth
043: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00022-of-00022.pth
044: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00001-of-00022.pth
045: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00002-of-00022.pth
046: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00003-of-00022.pth
047: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00004-of-00022.pth
048: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00005-of-00022.pth
049: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00006-of-00022.pth
050: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00007-of-00022.pth
051: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00008-of-00022.pth
052: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00009-of-00022.pth
053: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00010-of-00022.pth
054: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00011-of-00022.pth
055: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00012-of-00022.pth
056: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00013-of-00022.pth
057: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00014-of-00022.pth
058: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00015-of-00022.pth
059: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00016-of-00022.pth
060: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00017-of-00022.pth
061: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00018-of-00022.pth
062: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00019-of-00022.pth
063: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00020-of-00022.pth
064: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00021-of-00022.pth
065: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00022-of-00022.pth
066: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00001-of-00022.pth
067: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00002-of-00022.pth
068: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00003-of-00022.pth
069: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00004-of-00022.pth
070: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00005-of-00022.pth
071: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00006-of-00022.pth
072: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00007-of-00022.pth
073: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00008-of-00022.pth
074: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00009-of-00022.pth
075: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00010-of-00022.pth
076: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00011-of-00022.pth
077: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00012-of-00022.pth
078: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00013-of-00022.pth
079: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00014-of-00022.pth
080: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00015-of-00022.pth
081: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00016-of-00022.pth
082: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00017-of-00022.pth
083: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00018-of-00022.pth
084: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00019-of-00022.pth
085: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00020-of-00022.pth
086: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00021-of-00022.pth
087: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00022-of-00022.pth
088: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00001-of-00022.pth
089: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00002-of-00022.pth
090: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00003-of-00022.pth
091: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00004-of-00022.pth
092: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00005-of-00022.pth
093: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00006-of-00022.pth
094: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00007-of-00022.pth
095: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00008-of-00022.pth
096: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00009-of-00022.pth
097: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00010-of-00022.pth
098: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00011-of-00022.pth
099: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00012-of-00022.pth
100: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00013-of-00022.pth
101: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00014-of-00022.pth
102: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00015-of-00022.pth
103: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00016-of-00022.pth
104: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00017-of-00022.pth
105: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00018-of-00022.pth
106: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00019-of-00022.pth
107: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00020-of-00022.pth
108: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00021-of-00022.pth
109: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00022-of-00022.pth
110: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00001-of-00022.pth
111: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00002-of-00022.pth
112: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00003-of-00022.pth
113: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00004-of-00022.pth
114: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00005-of-00022.pth
115: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00006-of-00022.pth
116: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00007-of-00022.pth
117: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00008-of-00022.pth
118: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00009-of-00022.pth
119: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00010-of-00022.pth
120: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00011-of-00022.pth
121: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00012-of-00022.pth
122: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00013-of-00022.pth
123: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00014-of-00022.pth
124: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00015-of-00022.pth
125: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00016-of-00022.pth
126: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00017-of-00022.pth
127: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00018-of-00022.pth
128: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00019-of-00022.pth
129: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00020-of-00022.pth
130: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00021-of-00022.pth
131: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00022-of-00022.pth
132: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00001-of-00022.pth
133: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00002-of-00022.pth
134: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00003-of-00022.pth
135: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00004-of-00022.pth
136: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00005-of-00022.pth
137: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00006-of-00022.pth
138: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00007-of-00022.pth
139: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00008-of-00022.pth
140: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00009-of-00022.pth
141: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00010-of-00022.pth
142: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00011-of-00022.pth
143: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00012-of-00022.pth
144: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00013-of-00022.pth
145: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00014-of-00022.pth
146: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00015-of-00022.pth
147: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00016-of-00022.pth
148: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00017-of-00022.pth
149: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00018-of-00022.pth
150: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00019-of-00022.pth
151: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00020-of-00022.pth
152: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00021-of-00022.pth
153: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00022-of-00022.pth
154: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00001-of-00022.pth
155: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00002-of-00022.pth
156: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00003-of-00022.pth
157: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00004-of-00022.pth
158: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00005-of-00022.pth
159: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00006-of-00022.pth
160: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00007-of-00022.pth
161: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00008-of-00022.pth
162: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00009-of-00022.pth
163: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00010-of-00022.pth
164: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00011-of-00022.pth
165: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00012-of-00022.pth
166: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00013-of-00022.pth
167: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00014-of-00022.pth
168: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00015-of-00022.pth
169: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00016-of-00022.pth
170: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00017-of-00022.pth
171: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00018-of-00022.pth
172: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00019-of-00022.pth
173: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00020-of-00022.pth
174: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00021-of-00022.pth
175: models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00022-of-00022.pth

-----------------------------------------------------------------------------
0 params in total.
0 bytes in total.
-----------------------------------------------------------------------------

[1/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00001-of-00022.pth
   0 :  262668288 : tok_embeddings.weight               : [16032, 16384]  : torch.bfloat16
   1 :   33554432 : layers.0.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   33554432 : layers.0.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   5 :  109051904 : layers.0.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   6 :  109051904 : layers.0.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   7 :  109051904 : layers.0.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   33554432 : layers.1.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   33554432 : layers.1.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  14 :  109051904 : layers.1.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  15 :  109051904 : layers.1.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  16 :  109051904 : layers.1.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   33554432 : layers.2.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   33554432 : layers.2.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  23 :  109051904 : layers.2.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  24 :  109051904 : layers.2.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  25 :  109051904 : layers.2.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   33554432 : layers.3.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   33554432 : layers.3.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  32 :  109051904 : layers.3.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  33 :  109051904 : layers.3.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  34 :  109051904 : layers.3.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   33554432 : layers.4.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   33554432 : layers.4.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  41 :  109051904 : layers.4.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  42 :  109051904 : layers.4.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  43 :  109051904 : layers.4.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   33554432 : layers.5.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   33554432 : layers.5.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  50 :  109051904 : layers.5.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
2435481600 params in total.
4870963200 bytes in total.
35.30 sec, 35.30 sec, 131.61 MB/s
-----------------------------------------------------------------------------

[2/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00002-of-00022.pth
   0 :  109051904 : layers.5.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.5.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
   3 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
   4 :   33554432 : layers.6.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.6.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.6.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.6.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.6.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  12 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  13 :   33554432 : layers.7.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.7.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.7.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.7.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.7.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  21 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  22 :   33554432 : layers.8.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.8.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.8.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.8.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.8.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  30 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  31 :   33554432 : layers.9.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.9.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.9.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.9.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.9.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  39 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  40 :   33554432 : layers.10.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.10.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.10.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.10.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.10.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.11.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.11.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.11.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
4826431488 params in total.
9652862976 bytes in total.
74.98 sec, 39.68 sec, 122.78 MB/s
-----------------------------------------------------------------------------

[3/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00003-of-00022.pth
   0 :  109051904 : layers.11.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.11.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.12.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.12.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.12.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.12.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.12.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.13.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.13.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.13.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.13.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.13.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.14.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.14.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.14.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.14.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.14.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.15.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.15.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.15.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.15.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.15.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.16.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.16.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.16.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.16.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.16.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.17.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.17.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.17.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
7217381376 params in total.
14434762752 bytes in total.
118.41 sec, 43.44 sec, 116.25 MB/s
-----------------------------------------------------------------------------

[4/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00004-of-00022.pth
   0 :  109051904 : layers.17.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.17.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.18.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.18.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.18.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.18.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.18.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.19.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.19.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.19.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.19.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.19.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.20.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.20.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.20.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.20.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.20.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.21.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.21.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.21.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.21.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.21.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.22.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.22.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.22.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.22.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.22.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.23.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.23.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.23.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
9608331264 params in total.
19216662528 bytes in total.
161.65 sec, 43.24 sec, 113.37 MB/s
-----------------------------------------------------------------------------

[5/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00005-of-00022.pth
   0 :  109051904 : layers.23.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.23.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.24.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.24.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.24.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.24.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.24.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.25.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.25.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.25.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.25.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.25.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.26.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.26.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.26.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.26.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.26.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.27.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.27.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.27.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.27.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.27.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.28.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.28.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.28.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.28.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.28.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.29.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.29.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.29.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
11999281152 params in total.
23998562304 bytes in total.
204.98 sec, 43.33 sec, 111.65 MB/s
-----------------------------------------------------------------------------

[6/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00006-of-00022.pth
   0 :  109051904 : layers.29.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.29.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.30.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.30.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.30.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.30.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.30.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.31.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.31.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.31.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.31.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.31.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.32.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.32.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.32.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.32.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.32.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.33.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.33.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.33.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.33.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.33.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.34.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.34.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.34.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.34.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.34.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.35.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.35.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.35.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
14390231040 params in total.
28780462080 bytes in total.
248.54 sec, 43.56 sec, 110.44 MB/s
-----------------------------------------------------------------------------

[7/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00007-of-00022.pth
   0 :  109051904 : layers.35.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.35.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.36.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.36.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.36.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.36.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.36.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.37.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.37.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.37.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.37.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.37.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.38.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.38.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.38.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.38.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.38.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.39.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.39.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.39.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.39.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.39.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.40.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.40.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.40.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.40.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.40.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.41.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.41.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.41.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
16781180928 params in total.
33562361856 bytes in total.
292.39 sec, 43.85 sec, 109.47 MB/s
-----------------------------------------------------------------------------

[8/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00008-of-00022.pth
   0 :  109051904 : layers.41.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.41.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.42.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.42.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.42.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.42.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.42.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.43.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.43.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.43.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.43.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.43.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.44.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.44.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.44.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.44.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.44.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.45.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.45.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.45.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.45.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.45.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.46.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.46.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.46.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.46.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.46.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.47.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.47.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.47.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
19172130816 params in total.
38344261632 bytes in total.
336.82 sec, 44.43 sec, 108.57 MB/s
-----------------------------------------------------------------------------

[9/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00009-of-00022.pth
   0 :  109051904 : layers.47.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.47.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.48.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.48.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.48.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.48.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.48.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.49.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.49.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.49.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.49.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.49.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.50.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.50.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.50.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.50.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.50.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.51.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.51.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.51.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.51.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.51.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.52.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.52.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.52.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.52.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.52.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.53.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.53.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.53.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
21563080704 params in total.
43126161408 bytes in total.
380.81 sec, 43.99 sec, 108.00 MB/s
-----------------------------------------------------------------------------

[10/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00010-of-00022.pth
   0 :  109051904 : layers.53.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.53.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.54.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.54.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.54.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.54.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.54.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.55.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.55.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.55.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.55.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.55.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.56.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.56.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.56.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.56.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.56.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.57.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.57.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.57.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.57.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.57.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.58.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.58.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.58.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.58.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.58.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.59.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.59.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.59.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
23954030592 params in total.
47908061184 bytes in total.
424.54 sec, 43.73 sec, 107.62 MB/s
-----------------------------------------------------------------------------

[11/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00011-of-00022.pth
   0 :  109051904 : layers.59.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.59.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.60.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.60.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.60.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.60.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.60.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.61.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.61.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.61.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.61.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.61.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.62.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.62.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.62.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.62.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.62.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.63.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.63.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.63.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.63.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.63.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.64.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.64.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.64.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.64.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.64.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.65.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.65.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.65.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
26344980480 params in total.
52689960960 bytes in total.
468.75 sec, 44.21 sec, 107.20 MB/s
-----------------------------------------------------------------------------

[12/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00012-of-00022.pth
   0 :  109051904 : layers.65.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.65.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.66.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.66.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.66.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.66.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.66.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.67.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.67.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.67.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.67.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.67.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.68.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.68.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.68.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.68.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.68.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.69.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.69.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.69.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.69.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.69.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.70.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.70.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.70.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.70.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.70.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.71.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.71.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.71.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
28735930368 params in total.
57471860736 bytes in total.
513.22 sec, 44.47 sec, 106.79 MB/s
-----------------------------------------------------------------------------

[13/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00013-of-00022.pth
   0 :  109051904 : layers.71.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.71.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.72.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.72.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.72.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.72.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.72.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.73.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.73.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.73.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.73.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.73.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.74.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.74.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.74.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.74.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.74.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.75.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.75.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.75.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.75.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.75.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.76.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.76.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.76.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.76.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.76.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.77.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.77.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.77.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
31126880256 params in total.
62253760512 bytes in total.
557.08 sec, 43.86 sec, 106.57 MB/s
-----------------------------------------------------------------------------

[14/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00014-of-00022.pth
   0 :  109051904 : layers.77.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.77.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.78.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.78.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.78.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.78.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.78.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.79.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.79.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.79.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.79.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.79.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.80.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.80.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.80.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.80.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.80.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.81.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.81.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.81.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.81.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.81.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.82.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.82.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.82.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.82.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.82.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.83.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.83.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.83.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
33517830144 params in total.
67035660288 bytes in total.
601.02 sec, 43.94 sec, 106.37 MB/s
-----------------------------------------------------------------------------

[15/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00015-of-00022.pth
   0 :  109051904 : layers.83.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.83.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.84.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.84.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.84.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.84.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.84.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.85.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.85.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.85.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.85.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.85.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.86.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.86.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.86.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.86.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.86.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.87.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.87.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.87.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.87.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.87.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.88.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.88.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.88.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.88.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.88.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.89.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.89.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.89.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
35908780032 params in total.
71817560064 bytes in total.
644.89 sec, 43.87 sec, 106.21 MB/s
-----------------------------------------------------------------------------

[16/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00016-of-00022.pth
   0 :  109051904 : layers.89.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.89.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.90.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.90.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.90.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.90.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.90.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.91.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.91.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.91.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.91.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.91.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.92.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.92.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.92.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.92.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.92.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.93.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.93.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.93.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.93.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.93.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.94.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.94.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.94.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.94.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.94.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.95.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.95.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.95.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
38299729920 params in total.
76599459840 bytes in total.
688.73 sec, 43.84 sec, 106.07 MB/s
-----------------------------------------------------------------------------

[17/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00017-of-00022.pth
   0 :  109051904 : layers.95.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.95.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.96.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.96.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.96.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.96.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.96.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.97.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.97.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.97.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.97.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.97.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.98.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.98.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.98.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.98.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.98.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.99.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.99.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.99.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.99.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.99.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.100.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.100.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.100.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.100.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.100.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.101.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.101.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.101.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
40690679808 params in total.
81381359616 bytes in total.
732.79 sec, 44.06 sec, 105.91 MB/s
-----------------------------------------------------------------------------

[18/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00018-of-00022.pth
   0 :  109051904 : layers.101.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.101.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.102.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.102.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.102.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.102.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.102.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.103.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.103.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.103.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.103.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.103.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.104.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.104.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.104.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.104.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.104.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.105.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.105.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.105.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.105.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.105.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.106.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.106.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.106.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.106.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.106.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.107.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.107.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.107.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
43081629696 params in total.
86163259392 bytes in total.
776.63 sec, 43.84 sec, 105.81 MB/s
-----------------------------------------------------------------------------

[19/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00019-of-00022.pth
   0 :  109051904 : layers.107.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.107.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.108.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.108.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.108.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.108.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.108.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.109.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.109.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.109.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.109.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.109.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.110.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.110.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.110.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.110.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.110.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.111.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.111.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.111.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.111.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.111.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.112.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.112.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.112.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.112.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.112.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.113.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.113.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.113.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
45472579584 params in total.
90945159168 bytes in total.
820.57 sec, 43.95 sec, 105.70 MB/s
-----------------------------------------------------------------------------

[20/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00020-of-00022.pth
   0 :  109051904 : layers.113.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.113.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.114.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.114.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.114.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.114.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.114.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.115.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.115.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.115.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.115.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.115.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.116.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.116.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.116.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.116.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.116.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.117.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.117.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.117.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.117.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.117.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.118.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.118.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.118.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.118.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.118.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.119.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.119.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.119.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
47863529472 params in total.
95727058944 bytes in total.
865.06 sec, 44.49 sec, 105.53 MB/s
-----------------------------------------------------------------------------

[21/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00021-of-00022.pth
   0 :  109051904 : layers.119.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.119.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.120.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.120.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.120.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.120.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.120.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.121.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.121.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.121.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.121.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.121.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.122.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.122.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.122.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.122.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.122.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.123.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.123.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.123.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.123.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.123.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.124.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.124.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.124.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.124.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.124.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.125.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.125.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.125.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
50254479360 params in total.
100508958720 bytes in total.
909.85 sec, 44.78 sec, 105.35 MB/s
-----------------------------------------------------------------------------

[22/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.00/consolidated-00022-of-00022.pth
   0 :  109051904 : layers.125.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.125.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
   5 :  262668288 : output.weight                       : [16032, 16384]  : torch.bfloat16

-----------------------------------------------------------------------------
50735300608 params in total.
101470601216 bytes in total.
919.29 sec, 9.45 sec, 105.27 MB/s
-----------------------------------------------------------------------------

[23/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00001-of-00022.pth
   0 :  262668288 : tok_embeddings.weight               : [16032, 16384]  : torch.bfloat16
   1 :   33554432 : layers.0.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   33554432 : layers.0.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   5 :  109051904 : layers.0.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   6 :  109051904 : layers.0.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   7 :  109051904 : layers.0.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   33554432 : layers.1.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   33554432 : layers.1.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  14 :  109051904 : layers.1.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  15 :  109051904 : layers.1.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  16 :  109051904 : layers.1.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   33554432 : layers.2.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   33554432 : layers.2.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  23 :  109051904 : layers.2.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  24 :  109051904 : layers.2.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  25 :  109051904 : layers.2.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   33554432 : layers.3.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   33554432 : layers.3.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  32 :  109051904 : layers.3.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  33 :  109051904 : layers.3.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  34 :  109051904 : layers.3.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   33554432 : layers.4.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   33554432 : layers.4.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  41 :  109051904 : layers.4.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  42 :  109051904 : layers.4.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  43 :  109051904 : layers.4.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   33554432 : layers.5.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   33554432 : layers.5.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  50 :  109051904 : layers.5.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
53170782208 params in total.
106341564416 bytes in total.
964.24 sec, 44.95 sec, 105.18 MB/s
-----------------------------------------------------------------------------

[24/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00002-of-00022.pth
   0 :  109051904 : layers.5.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.5.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
   3 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
   4 :   33554432 : layers.6.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.6.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.6.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.6.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.6.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  12 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  13 :   33554432 : layers.7.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.7.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.7.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.7.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.7.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  21 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  22 :   33554432 : layers.8.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.8.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.8.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.8.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.8.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  30 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  31 :   33554432 : layers.9.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.9.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.9.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.9.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.9.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  39 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  40 :   33554432 : layers.10.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.10.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.10.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.10.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.10.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.11.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.11.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.11.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
55561732096 params in total.
111123464192 bytes in total.
1009.12 sec, 44.88 sec, 105.02 MB/s
-----------------------------------------------------------------------------

[25/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00003-of-00022.pth
   0 :  109051904 : layers.11.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.11.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.12.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.12.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.12.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.12.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.12.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.13.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.13.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.13.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.13.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.13.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.14.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.14.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.14.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.14.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.14.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.15.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.15.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.15.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.15.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.15.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.16.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.16.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.16.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.16.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.16.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.17.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.17.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.17.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
57952681984 params in total.
115905363968 bytes in total.
1053.85 sec, 44.73 sec, 104.89 MB/s
-----------------------------------------------------------------------------

[26/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00004-of-00022.pth
   0 :  109051904 : layers.17.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.17.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.18.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.18.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.18.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.18.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.18.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.19.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.19.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.19.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.19.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.19.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.20.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.20.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.20.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.20.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.20.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.21.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.21.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.21.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.21.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.21.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.22.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.22.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.22.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.22.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.22.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.23.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.23.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.23.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
60343631872 params in total.
120687263744 bytes in total.
1098.82 sec, 44.97 sec, 104.75 MB/s
-----------------------------------------------------------------------------

[27/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00005-of-00022.pth
   0 :  109051904 : layers.23.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.23.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.24.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.24.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.24.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.24.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.24.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.25.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.25.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.25.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.25.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.25.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.26.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.26.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.26.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.26.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.26.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.27.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.27.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.27.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.27.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.27.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.28.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.28.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.28.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.28.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.28.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.29.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.29.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.29.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
62734581760 params in total.
125469163520 bytes in total.
1143.41 sec, 44.59 sec, 104.65 MB/s
-----------------------------------------------------------------------------

[28/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00006-of-00022.pth
   0 :  109051904 : layers.29.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.29.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.30.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.30.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.30.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.30.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.30.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.31.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.31.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.31.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.31.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.31.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.32.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.32.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.32.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.32.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.32.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.33.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.33.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.33.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.33.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.33.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.34.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.34.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.34.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.34.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.34.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.35.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.35.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.35.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
65125531648 params in total.
130251063296 bytes in total.
1188.01 sec, 44.60 sec, 104.56 MB/s
-----------------------------------------------------------------------------

[29/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00007-of-00022.pth
   0 :  109051904 : layers.35.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.35.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.36.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.36.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.36.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.36.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.36.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.37.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.37.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.37.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.37.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.37.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.38.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.38.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.38.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.38.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.38.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.39.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.39.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.39.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.39.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.39.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.40.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.40.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.40.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.40.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.40.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.41.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.41.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.41.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
67516481536 params in total.
135032963072 bytes in total.
1233.00 sec, 44.99 sec, 104.44 MB/s
-----------------------------------------------------------------------------

[30/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00008-of-00022.pth
   0 :  109051904 : layers.41.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.41.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.42.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.42.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.42.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.42.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.42.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.43.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.43.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.43.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.43.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.43.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.44.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.44.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.44.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.44.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.44.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.45.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.45.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.45.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.45.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.45.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.46.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.46.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.46.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.46.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.46.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.47.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.47.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.47.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
69907431424 params in total.
139814862848 bytes in total.
1277.92 sec, 44.92 sec, 104.34 MB/s
-----------------------------------------------------------------------------

[31/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00009-of-00022.pth
   0 :  109051904 : layers.47.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.47.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.48.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.48.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.48.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.48.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.48.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.49.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.49.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.49.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.49.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.49.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.50.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.50.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.50.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.50.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.50.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.51.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.51.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.51.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.51.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.51.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.52.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.52.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.52.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.52.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.52.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.53.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.53.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.53.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
72298381312 params in total.
144596762624 bytes in total.
1322.33 sec, 44.40 sec, 104.28 MB/s
-----------------------------------------------------------------------------

[32/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00010-of-00022.pth
   0 :  109051904 : layers.53.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.53.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.54.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.54.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.54.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.54.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.54.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.55.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.55.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.55.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.55.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.55.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.56.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.56.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.56.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.56.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.56.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.57.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.57.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.57.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.57.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.57.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.58.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.58.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.58.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.58.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.58.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.59.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.59.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.59.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
74689331200 params in total.
149378662400 bytes in total.
1366.98 sec, 44.65 sec, 104.21 MB/s
-----------------------------------------------------------------------------

[33/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00011-of-00022.pth
   0 :  109051904 : layers.59.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.59.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.60.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.60.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.60.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.60.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.60.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.61.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.61.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.61.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.61.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.61.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.62.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.62.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.62.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.62.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.62.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.63.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.63.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.63.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.63.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.63.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.64.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.64.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.64.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.64.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.64.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.65.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.65.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.65.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
77080281088 params in total.
154160562176 bytes in total.
1411.84 sec, 44.86 sec, 104.13 MB/s
-----------------------------------------------------------------------------

[34/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00012-of-00022.pth
   0 :  109051904 : layers.65.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.65.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.66.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.66.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.66.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.66.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.66.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.67.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.67.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.67.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.67.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.67.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.68.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.68.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.68.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.68.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.68.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.69.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.69.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.69.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.69.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.69.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.70.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.70.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.70.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.70.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.70.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.71.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.71.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.71.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
79471230976 params in total.
158942461952 bytes in total.
1456.60 sec, 44.76 sec, 104.06 MB/s
-----------------------------------------------------------------------------

[35/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00013-of-00022.pth
   0 :  109051904 : layers.71.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.71.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.72.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.72.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.72.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.72.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.72.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.73.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.73.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.73.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.73.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.73.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.74.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.74.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.74.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.74.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.74.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.75.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.75.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.75.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.75.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.75.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.76.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.76.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.76.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.76.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.76.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.77.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.77.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.77.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
81862180864 params in total.
163724361728 bytes in total.
1501.72 sec, 45.12 sec, 103.97 MB/s
-----------------------------------------------------------------------------

[36/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00014-of-00022.pth
   0 :  109051904 : layers.77.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.77.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.78.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.78.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.78.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.78.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.78.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.79.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.79.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.79.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.79.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.79.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.80.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.80.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.80.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.80.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.80.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.81.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.81.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.81.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.81.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.81.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.82.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.82.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.82.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.82.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.82.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.83.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.83.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.83.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
84253130752 params in total.
168506261504 bytes in total.
1546.96 sec, 45.24 sec, 103.88 MB/s
-----------------------------------------------------------------------------

[37/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00015-of-00022.pth
   0 :  109051904 : layers.83.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.83.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.84.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.84.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.84.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.84.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.84.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.85.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.85.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.85.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.85.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.85.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.86.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.86.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.86.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.86.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.86.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.87.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.87.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.87.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.87.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.87.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.88.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.88.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.88.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.88.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.88.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.89.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.89.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.89.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
86644080640 params in total.
173288161280 bytes in total.
1591.74 sec, 44.78 sec, 103.82 MB/s
-----------------------------------------------------------------------------

[38/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00016-of-00022.pth
   0 :  109051904 : layers.89.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.89.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.90.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.90.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.90.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.90.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.90.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.91.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.91.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.91.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.91.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.91.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.92.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.92.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.92.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.92.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.92.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.93.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.93.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.93.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.93.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.93.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.94.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.94.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.94.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.94.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.94.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.95.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.95.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.95.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
89035030528 params in total.
178070061056 bytes in total.
1636.90 sec, 45.15 sec, 103.75 MB/s
-----------------------------------------------------------------------------

[39/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00017-of-00022.pth
   0 :  109051904 : layers.95.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.95.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.96.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.96.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.96.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.96.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.96.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.97.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.97.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.97.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.97.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.97.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.98.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.98.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.98.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.98.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.98.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.99.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.99.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.99.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.99.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.99.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.100.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.100.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.100.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.100.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.100.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.101.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.101.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.101.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
91425980416 params in total.
182851960832 bytes in total.
1682.21 sec, 45.31 sec, 103.66 MB/s
-----------------------------------------------------------------------------

[40/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00018-of-00022.pth
   0 :  109051904 : layers.101.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.101.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.102.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.102.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.102.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.102.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.102.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.103.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.103.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.103.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.103.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.103.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.104.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.104.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.104.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.104.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.104.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.105.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.105.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.105.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.105.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.105.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.106.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.106.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.106.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.106.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.106.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.107.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.107.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.107.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
93816930304 params in total.
187633860608 bytes in total.
1727.60 sec, 45.40 sec, 103.58 MB/s
-----------------------------------------------------------------------------

[41/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00019-of-00022.pth
   0 :  109051904 : layers.107.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.107.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.108.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.108.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.108.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.108.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.108.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.109.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.109.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.109.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.109.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.109.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.110.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.110.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.110.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.110.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.110.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.111.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.111.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.111.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.111.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.111.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.112.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.112.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.112.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.112.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.112.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.113.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.113.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.113.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
96207880192 params in total.
192415760384 bytes in total.
1772.74 sec, 45.14 sec, 103.51 MB/s
-----------------------------------------------------------------------------

[42/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00020-of-00022.pth
   0 :  109051904 : layers.113.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.113.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.114.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.114.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.114.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.114.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.114.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.115.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.115.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.115.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.115.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.115.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.116.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.116.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.116.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.116.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.116.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.117.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.117.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.117.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.117.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.117.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.118.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.118.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.118.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.118.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.118.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.119.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.119.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.119.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
98598830080 params in total.
197197660160 bytes in total.
1817.85 sec, 45.11 sec, 103.45 MB/s
-----------------------------------------------------------------------------

[43/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00021-of-00022.pth
   0 :  109051904 : layers.119.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.119.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.120.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.120.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.120.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.120.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.120.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.121.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.121.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.121.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.121.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.121.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.122.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.122.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.122.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.122.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.122.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.123.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.123.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.123.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.123.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.123.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.124.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.124.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.124.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.124.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.124.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.125.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.125.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.125.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
100989779968 params in total.
201979559936 bytes in total.
1863.12 sec, 45.27 sec, 103.39 MB/s
-----------------------------------------------------------------------------

[44/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.01/consolidated-00022-of-00022.pth
   0 :  109051904 : layers.125.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.125.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
   5 :  262668288 : output.weight                       : [16032, 16384]  : torch.bfloat16

-----------------------------------------------------------------------------
101470601216 params in total.
202941202432 bytes in total.
1872.68 sec, 9.56 sec, 103.35 MB/s
-----------------------------------------------------------------------------

[45/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00001-of-00022.pth
   0 :  262668288 : tok_embeddings.weight               : [16032, 16384]  : torch.bfloat16
   1 :   33554432 : layers.0.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   33554432 : layers.0.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   5 :  109051904 : layers.0.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   6 :  109051904 : layers.0.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   7 :  109051904 : layers.0.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   33554432 : layers.1.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   33554432 : layers.1.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  14 :  109051904 : layers.1.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  15 :  109051904 : layers.1.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  16 :  109051904 : layers.1.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   33554432 : layers.2.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   33554432 : layers.2.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  23 :  109051904 : layers.2.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  24 :  109051904 : layers.2.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  25 :  109051904 : layers.2.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   33554432 : layers.3.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   33554432 : layers.3.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  32 :  109051904 : layers.3.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  33 :  109051904 : layers.3.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  34 :  109051904 : layers.3.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   33554432 : layers.4.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   33554432 : layers.4.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  41 :  109051904 : layers.4.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  42 :  109051904 : layers.4.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  43 :  109051904 : layers.4.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   33554432 : layers.5.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   33554432 : layers.5.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  50 :  109051904 : layers.5.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
103906082816 params in total.
207812165632 bytes in total.
1918.80 sec, 46.13 sec, 103.29 MB/s
-----------------------------------------------------------------------------

[46/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00002-of-00022.pth
   0 :  109051904 : layers.5.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.5.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
   3 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
   4 :   33554432 : layers.6.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.6.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.6.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.6.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.6.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  12 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  13 :   33554432 : layers.7.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.7.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.7.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.7.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.7.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  21 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  22 :   33554432 : layers.8.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.8.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.8.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.8.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.8.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  30 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  31 :   33554432 : layers.9.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.9.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.9.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.9.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.9.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  39 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  40 :   33554432 : layers.10.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.10.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.10.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.10.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.10.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.11.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.11.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.11.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
106297032704 params in total.
212594065408 bytes in total.
1963.90 sec, 45.09 sec, 103.24 MB/s
-----------------------------------------------------------------------------

[47/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00003-of-00022.pth
   0 :  109051904 : layers.11.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.11.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.12.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.12.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.12.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.12.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.12.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.13.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.13.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.13.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.13.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.13.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.14.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.14.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.14.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.14.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.14.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.15.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.15.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.15.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.15.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.15.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.16.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.16.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.16.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.16.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.16.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.17.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.17.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.17.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
108687982592 params in total.
217375965184 bytes in total.
2009.46 sec, 45.56 sec, 103.17 MB/s
-----------------------------------------------------------------------------

[48/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00004-of-00022.pth
   0 :  109051904 : layers.17.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.17.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.18.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.18.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.18.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.18.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.18.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.19.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.19.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.19.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.19.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.19.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.20.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.20.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.20.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.20.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.20.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.21.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.21.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.21.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.21.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.21.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.22.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.22.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.22.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.22.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.22.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.23.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.23.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.23.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
111078932480 params in total.
222157864960 bytes in total.
2054.86 sec, 45.40 sec, 103.11 MB/s
-----------------------------------------------------------------------------

[49/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00005-of-00022.pth
   0 :  109051904 : layers.23.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.23.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.24.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.24.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.24.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.24.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.24.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.25.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.25.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.25.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.25.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.25.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.26.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.26.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.26.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.26.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.26.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.27.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.27.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.27.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.27.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.27.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.28.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.28.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.28.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.28.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.28.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.29.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.29.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.29.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
113469882368 params in total.
226939764736 bytes in total.
2100.38 sec, 45.52 sec, 103.04 MB/s
-----------------------------------------------------------------------------

[50/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00006-of-00022.pth
   0 :  109051904 : layers.29.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.29.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.30.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.30.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.30.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.30.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.30.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.31.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.31.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.31.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.31.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.31.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.32.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.32.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.32.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.32.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.32.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.33.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.33.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.33.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.33.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.33.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.34.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.34.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.34.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.34.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.34.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.35.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.35.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.35.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
115860832256 params in total.
231721664512 bytes in total.
2146.00 sec, 45.62 sec, 102.98 MB/s
-----------------------------------------------------------------------------

[51/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00007-of-00022.pth
   0 :  109051904 : layers.35.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.35.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.36.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.36.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.36.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.36.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.36.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.37.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.37.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.37.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.37.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.37.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.38.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.38.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.38.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.38.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.38.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.39.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.39.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.39.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.39.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.39.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.40.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.40.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.40.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.40.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.40.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.41.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.41.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.41.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
118251782144 params in total.
236503564288 bytes in total.
2191.68 sec, 45.69 sec, 102.91 MB/s
-----------------------------------------------------------------------------

[52/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00008-of-00022.pth
   0 :  109051904 : layers.41.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.41.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.42.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.42.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.42.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.42.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.42.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.43.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.43.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.43.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.43.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.43.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.44.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.44.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.44.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.44.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.44.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.45.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.45.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.45.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.45.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.45.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.46.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.46.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.46.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.46.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.46.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.47.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.47.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.47.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
120642732032 params in total.
241285464064 bytes in total.
2237.35 sec, 45.67 sec, 102.85 MB/s
-----------------------------------------------------------------------------

[53/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00009-of-00022.pth
   0 :  109051904 : layers.47.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.47.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.48.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.48.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.48.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.48.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.48.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.49.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.49.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.49.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.49.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.49.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.50.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.50.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.50.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.50.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.50.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.51.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.51.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.51.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.51.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.51.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.52.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.52.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.52.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.52.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.52.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.53.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.53.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.53.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
123033681920 params in total.
246067363840 bytes in total.
2283.35 sec, 46.01 sec, 102.77 MB/s
-----------------------------------------------------------------------------

[54/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00010-of-00022.pth
   0 :  109051904 : layers.53.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.53.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.54.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.54.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.54.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.54.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.54.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.55.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.55.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.55.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.55.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.55.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.56.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.56.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.56.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.56.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.56.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.57.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.57.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.57.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.57.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.57.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.58.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.58.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.58.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.58.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.58.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.59.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.59.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.59.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
125424631808 params in total.
250849263616 bytes in total.
2329.40 sec, 46.05 sec, 102.70 MB/s
-----------------------------------------------------------------------------

[55/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00011-of-00022.pth
   0 :  109051904 : layers.59.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.59.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.60.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.60.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.60.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.60.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.60.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.61.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.61.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.61.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.61.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.61.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.62.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.62.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.62.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.62.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.62.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.63.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.63.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.63.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.63.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.63.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.64.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.64.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.64.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.64.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.64.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.65.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.65.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.65.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
127815581696 params in total.
255631163392 bytes in total.
2375.06 sec, 45.66 sec, 102.65 MB/s
-----------------------------------------------------------------------------

[56/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00012-of-00022.pth
   0 :  109051904 : layers.65.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.65.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.66.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.66.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.66.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.66.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.66.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.67.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.67.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.67.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.67.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.67.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.68.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.68.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.68.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.68.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.68.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.69.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.69.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.69.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.69.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.69.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.70.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.70.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.70.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.70.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.70.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.71.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.71.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.71.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
130206531584 params in total.
260413063168 bytes in total.
2420.94 sec, 45.88 sec, 102.58 MB/s
-----------------------------------------------------------------------------

[57/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00013-of-00022.pth
   0 :  109051904 : layers.71.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.71.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.72.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.72.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.72.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.72.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.72.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.73.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.73.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.73.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.73.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.73.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.74.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.74.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.74.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.74.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.74.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.75.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.75.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.75.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.75.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.75.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.76.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.76.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.76.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.76.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.76.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.77.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.77.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.77.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
132597481472 params in total.
265194962944 bytes in total.
2466.80 sec, 45.85 sec, 102.53 MB/s
-----------------------------------------------------------------------------

[58/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00014-of-00022.pth
   0 :  109051904 : layers.77.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.77.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.78.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.78.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.78.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.78.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.78.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.79.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.79.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.79.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.79.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.79.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.80.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.80.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.80.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.80.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.80.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.81.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.81.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.81.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.81.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.81.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.82.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.82.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.82.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.82.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.82.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.83.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.83.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.83.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
134988431360 params in total.
269976862720 bytes in total.
2512.47 sec, 45.67 sec, 102.48 MB/s
-----------------------------------------------------------------------------

[59/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00015-of-00022.pth
   0 :  109051904 : layers.83.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.83.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.84.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.84.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.84.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.84.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.84.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.85.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.85.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.85.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.85.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.85.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.86.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.86.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.86.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.86.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.86.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.87.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.87.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.87.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.87.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.87.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.88.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.88.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.88.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.88.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.88.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.89.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.89.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.89.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
137379381248 params in total.
274758762496 bytes in total.
2558.31 sec, 45.84 sec, 102.42 MB/s
-----------------------------------------------------------------------------

[60/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00016-of-00022.pth
   0 :  109051904 : layers.89.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.89.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.90.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.90.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.90.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.90.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.90.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.91.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.91.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.91.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.91.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.91.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.92.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.92.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.92.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.92.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.92.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.93.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.93.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.93.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.93.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.93.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.94.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.94.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.94.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.94.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.94.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.95.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.95.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.95.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
139770331136 params in total.
279540662272 bytes in total.
2603.92 sec, 45.60 sec, 102.38 MB/s
-----------------------------------------------------------------------------

[61/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00017-of-00022.pth
   0 :  109051904 : layers.95.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.95.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.96.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.96.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.96.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.96.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.96.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.97.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.97.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.97.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.97.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.97.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.98.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.98.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.98.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.98.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.98.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.99.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.99.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.99.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.99.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.99.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.100.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.100.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.100.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.100.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.100.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.101.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.101.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.101.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
142161281024 params in total.
284322562048 bytes in total.
2649.74 sec, 45.82 sec, 102.33 MB/s
-----------------------------------------------------------------------------

[62/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00018-of-00022.pth
   0 :  109051904 : layers.101.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.101.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.102.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.102.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.102.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.102.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.102.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.103.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.103.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.103.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.103.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.103.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.104.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.104.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.104.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.104.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.104.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.105.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.105.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.105.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.105.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.105.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.106.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.106.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.106.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.106.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.106.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.107.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.107.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.107.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
144552230912 params in total.
289104461824 bytes in total.
2696.52 sec, 46.78 sec, 102.25 MB/s
-----------------------------------------------------------------------------

[63/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00019-of-00022.pth
   0 :  109051904 : layers.107.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.107.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.108.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.108.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.108.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.108.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.108.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.109.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.109.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.109.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.109.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.109.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.110.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.110.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.110.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.110.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.110.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.111.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.111.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.111.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.111.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.111.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.112.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.112.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.112.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.112.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.112.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.113.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.113.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.113.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
146943180800 params in total.
293886361600 bytes in total.
2743.35 sec, 46.83 sec, 102.16 MB/s
-----------------------------------------------------------------------------

[64/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00020-of-00022.pth
   0 :  109051904 : layers.113.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.113.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.114.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.114.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.114.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.114.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.114.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.115.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.115.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.115.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.115.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.115.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.116.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.116.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.116.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.116.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.116.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.117.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.117.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.117.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.117.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.117.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.118.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.118.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.118.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.118.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.118.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.119.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.119.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.119.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
149334130688 params in total.
298668261376 bytes in total.
2790.41 sec, 47.06 sec, 102.08 MB/s
-----------------------------------------------------------------------------

[65/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00021-of-00022.pth
   0 :  109051904 : layers.119.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.119.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.120.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.120.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.120.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.120.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.120.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.121.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.121.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.121.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.121.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.121.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.122.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.122.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.122.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.122.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.122.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.123.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.123.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.123.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.123.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.123.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.124.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.124.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.124.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.124.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.124.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.125.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.125.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.125.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
151725080576 params in total.
303450161152 bytes in total.
2837.19 sec, 46.79 sec, 102.00 MB/s
-----------------------------------------------------------------------------

[66/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.02/consolidated-00022-of-00022.pth
   0 :  109051904 : layers.125.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.125.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
   5 :  262668288 : output.weight                       : [16032, 16384]  : torch.bfloat16

-----------------------------------------------------------------------------
152205901824 params in total.
304411803648 bytes in total.
2847.21 sec, 10.02 sec, 101.96 MB/s
-----------------------------------------------------------------------------

[67/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00001-of-00022.pth
   0 :  262668288 : tok_embeddings.weight               : [16032, 16384]  : torch.bfloat16
   1 :   33554432 : layers.0.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   33554432 : layers.0.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   5 :  109051904 : layers.0.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   6 :  109051904 : layers.0.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   7 :  109051904 : layers.0.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   33554432 : layers.1.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   33554432 : layers.1.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  14 :  109051904 : layers.1.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  15 :  109051904 : layers.1.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  16 :  109051904 : layers.1.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   33554432 : layers.2.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   33554432 : layers.2.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  23 :  109051904 : layers.2.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  24 :  109051904 : layers.2.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  25 :  109051904 : layers.2.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   33554432 : layers.3.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   33554432 : layers.3.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  32 :  109051904 : layers.3.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  33 :  109051904 : layers.3.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  34 :  109051904 : layers.3.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   33554432 : layers.4.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   33554432 : layers.4.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  41 :  109051904 : layers.4.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  42 :  109051904 : layers.4.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  43 :  109051904 : layers.4.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   33554432 : layers.5.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   33554432 : layers.5.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  50 :  109051904 : layers.5.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
154641383424 params in total.
309282766848 bytes in total.
2894.46 sec, 47.25 sec, 101.90 MB/s
-----------------------------------------------------------------------------

[68/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00002-of-00022.pth
   0 :  109051904 : layers.5.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.5.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
   3 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
   4 :   33554432 : layers.6.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.6.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.6.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.6.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.6.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  12 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  13 :   33554432 : layers.7.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.7.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.7.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.7.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.7.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  21 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  22 :   33554432 : layers.8.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.8.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.8.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.8.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.8.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  30 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  31 :   33554432 : layers.9.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.9.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.9.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.9.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.9.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  39 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  40 :   33554432 : layers.10.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.10.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.10.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.10.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.10.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.11.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.11.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.11.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
157032333312 params in total.
314064666624 bytes in total.
2941.28 sec, 46.82 sec, 101.83 MB/s
-----------------------------------------------------------------------------

[69/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00003-of-00022.pth
   0 :  109051904 : layers.11.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.11.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.12.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.12.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.12.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.12.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.12.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.13.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.13.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.13.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.13.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.13.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.14.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.14.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.14.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.14.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.14.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.15.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.15.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.15.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.15.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.15.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.16.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.16.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.16.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.16.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.16.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.17.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.17.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.17.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
159423283200 params in total.
318846566400 bytes in total.
2988.11 sec, 46.83 sec, 101.76 MB/s
-----------------------------------------------------------------------------

[70/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00004-of-00022.pth
   0 :  109051904 : layers.17.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.17.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.18.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.18.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.18.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.18.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.18.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.19.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.19.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.19.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.19.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.19.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.20.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.20.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.20.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.20.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.20.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.21.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.21.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.21.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.21.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.21.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.22.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.22.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.22.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.22.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.22.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.23.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.23.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.23.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
161814233088 params in total.
323628466176 bytes in total.
3035.00 sec, 46.90 sec, 101.69 MB/s
-----------------------------------------------------------------------------

[71/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00005-of-00022.pth
   0 :  109051904 : layers.23.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.23.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.24.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.24.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.24.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.24.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.24.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.25.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.25.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.25.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.25.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.25.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.26.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.26.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.26.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.26.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.26.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.27.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.27.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.27.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.27.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.27.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.28.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.28.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.28.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.28.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.28.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.29.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.29.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.29.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
164205182976 params in total.
328410365952 bytes in total.
3081.70 sec, 46.69 sec, 101.63 MB/s
-----------------------------------------------------------------------------

[72/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00006-of-00022.pth
   0 :  109051904 : layers.29.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.29.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.30.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.30.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.30.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.30.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.30.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.31.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.31.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.31.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.31.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.31.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.32.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.32.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.32.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.32.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.32.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.33.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.33.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.33.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.33.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.33.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.34.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.34.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.34.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.34.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.34.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.35.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.35.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.35.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
166596132864 params in total.
333192265728 bytes in total.
3128.67 sec, 46.97 sec, 101.56 MB/s
-----------------------------------------------------------------------------

[73/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00007-of-00022.pth
   0 :  109051904 : layers.35.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.35.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.36.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.36.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.36.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.36.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.36.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.37.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.37.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.37.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.37.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.37.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.38.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.38.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.38.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.38.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.38.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.39.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.39.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.39.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.39.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.39.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.40.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.40.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.40.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.40.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.40.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.41.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.41.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.41.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
168987082752 params in total.
337974165504 bytes in total.
3175.39 sec, 46.72 sec, 101.50 MB/s
-----------------------------------------------------------------------------

[74/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00008-of-00022.pth
   0 :  109051904 : layers.41.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.41.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.42.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.42.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.42.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.42.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.42.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.43.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.43.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.43.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.43.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.43.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.44.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.44.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.44.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.44.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.44.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.45.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.45.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.45.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.45.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.45.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.46.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.46.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.46.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.46.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.46.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.47.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.47.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.47.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
171378032640 params in total.
342756065280 bytes in total.
3222.32 sec, 46.92 sec, 101.44 MB/s
-----------------------------------------------------------------------------

[75/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00009-of-00022.pth
   0 :  109051904 : layers.47.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.47.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.48.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.48.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.48.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.48.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.48.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.49.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.49.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.49.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.49.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.49.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.50.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.50.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.50.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.50.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.50.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.51.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.51.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.51.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.51.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.51.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.52.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.52.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.52.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.52.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.52.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.53.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.53.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.53.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
173768982528 params in total.
347537965056 bytes in total.
3269.29 sec, 46.98 sec, 101.38 MB/s
-----------------------------------------------------------------------------

[76/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00010-of-00022.pth
   0 :  109051904 : layers.53.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.53.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.54.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.54.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.54.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.54.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.54.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.55.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.55.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.55.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.55.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.55.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.56.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.56.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.56.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.56.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.56.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.57.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.57.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.57.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.57.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.57.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.58.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.58.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.58.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.58.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.58.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.59.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.59.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.59.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
176159932416 params in total.
352319864832 bytes in total.
3316.94 sec, 47.65 sec, 101.30 MB/s
-----------------------------------------------------------------------------

[77/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00011-of-00022.pth
   0 :  109051904 : layers.59.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.59.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.60.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.60.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.60.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.60.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.60.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.61.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.61.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.61.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.61.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.61.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.62.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.62.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.62.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.62.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.62.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.63.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.63.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.63.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.63.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.63.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.64.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.64.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.64.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.64.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.64.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.65.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.65.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.65.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
178550882304 params in total.
357101764608 bytes in total.
3364.77 sec, 47.83 sec, 101.21 MB/s
-----------------------------------------------------------------------------

[78/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00012-of-00022.pth
   0 :  109051904 : layers.65.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.65.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.66.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.66.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.66.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.66.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.66.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.67.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.67.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.67.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.67.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.67.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.68.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.68.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.68.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.68.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.68.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.69.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.69.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.69.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.69.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.69.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.70.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.70.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.70.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.70.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.70.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.71.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.71.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.71.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
180941832192 params in total.
361883664384 bytes in total.
3412.83 sec, 48.06 sec, 101.12 MB/s
-----------------------------------------------------------------------------

[79/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00013-of-00022.pth
   0 :  109051904 : layers.71.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.71.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.72.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.72.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.72.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.72.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.72.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.73.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.73.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.73.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.73.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.73.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.74.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.74.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.74.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.74.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.74.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.75.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.75.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.75.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.75.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.75.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.76.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.76.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.76.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.76.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.76.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.77.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.77.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.77.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
183332782080 params in total.
366665564160 bytes in total.
3460.46 sec, 47.63 sec, 101.05 MB/s
-----------------------------------------------------------------------------

[80/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00014-of-00022.pth
   0 :  109051904 : layers.77.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.77.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.78.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.78.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.78.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.78.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.78.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.79.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.79.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.79.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.79.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.79.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.80.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.80.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.80.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.80.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.80.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.81.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.81.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.81.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.81.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.81.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.82.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.82.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.82.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.82.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.82.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.83.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.83.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.83.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
185723731968 params in total.
371447463936 bytes in total.
3508.59 sec, 48.13 sec, 100.96 MB/s
-----------------------------------------------------------------------------

[81/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00015-of-00022.pth
   0 :  109051904 : layers.83.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.83.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.84.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.84.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.84.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.84.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.84.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.85.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.85.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.85.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.85.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.85.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.86.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.86.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.86.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.86.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.86.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.87.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.87.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.87.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.87.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.87.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.88.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.88.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.88.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.88.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.88.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.89.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.89.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.89.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
188114681856 params in total.
376229363712 bytes in total.
3556.58 sec, 47.99 sec, 100.88 MB/s
-----------------------------------------------------------------------------

[82/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00016-of-00022.pth
   0 :  109051904 : layers.89.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.89.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.90.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.90.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.90.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.90.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.90.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.91.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.91.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.91.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.91.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.91.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.92.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.92.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.92.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.92.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.92.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.93.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.93.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.93.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.93.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.93.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.94.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.94.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.94.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.94.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.94.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.95.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.95.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.95.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
190505631744 params in total.
381011263488 bytes in total.
3604.15 sec, 47.57 sec, 100.82 MB/s
-----------------------------------------------------------------------------

[83/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00017-of-00022.pth
   0 :  109051904 : layers.95.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.95.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.96.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.96.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.96.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.96.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.96.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.97.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.97.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.97.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.97.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.97.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.98.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.98.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.98.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.98.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.98.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.99.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.99.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.99.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.99.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.99.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.100.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.100.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.100.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.100.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.100.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.101.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.101.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.101.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
192896581632 params in total.
385793163264 bytes in total.
3652.15 sec, 48.00 sec, 100.74 MB/s
-----------------------------------------------------------------------------

[84/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00018-of-00022.pth
   0 :  109051904 : layers.101.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.101.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.102.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.102.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.102.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.102.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.102.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.103.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.103.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.103.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.103.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.103.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.104.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.104.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.104.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.104.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.104.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.105.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.105.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.105.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.105.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.105.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.106.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.106.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.106.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.106.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.106.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.107.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.107.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.107.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
195287531520 params in total.
390575063040 bytes in total.
3700.13 sec, 47.97 sec, 100.67 MB/s
-----------------------------------------------------------------------------

[85/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00019-of-00022.pth
   0 :  109051904 : layers.107.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.107.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.108.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.108.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.108.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.108.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.108.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.109.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.109.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.109.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.109.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.109.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.110.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.110.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.110.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.110.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.110.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.111.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.111.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.111.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.111.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.111.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.112.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.112.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.112.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.112.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.112.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.113.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.113.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.113.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
197678481408 params in total.
395356962816 bytes in total.
3748.48 sec, 48.35 sec, 100.59 MB/s
-----------------------------------------------------------------------------

[86/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00020-of-00022.pth
   0 :  109051904 : layers.113.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.113.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.114.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.114.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.114.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.114.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.114.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.115.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.115.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.115.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.115.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.115.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.116.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.116.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.116.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.116.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.116.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.117.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.117.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.117.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.117.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.117.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.118.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.118.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.118.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.118.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.118.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.119.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.119.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.119.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
200069431296 params in total.
400138862592 bytes in total.
3796.81 sec, 48.33 sec, 100.51 MB/s
-----------------------------------------------------------------------------

[87/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00021-of-00022.pth
   0 :  109051904 : layers.119.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.119.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.120.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.120.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.120.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.120.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.120.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.121.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.121.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.121.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.121.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.121.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.122.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.122.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.122.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.122.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.122.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.123.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.123.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.123.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.123.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.123.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.124.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.124.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.124.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.124.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.124.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.125.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.125.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.125.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
202460381184 params in total.
404920762368 bytes in total.
3844.90 sec, 48.09 sec, 100.44 MB/s
-----------------------------------------------------------------------------

[88/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.03/consolidated-00022-of-00022.pth
   0 :  109051904 : layers.125.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.125.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
   5 :  262668288 : output.weight                       : [16032, 16384]  : torch.bfloat16

-----------------------------------------------------------------------------
202941202432 params in total.
405882404864 bytes in total.
3855.02 sec, 10.12 sec, 100.41 MB/s
-----------------------------------------------------------------------------

[89/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00001-of-00022.pth
   0 :  262668288 : tok_embeddings.weight               : [16032, 16384]  : torch.bfloat16
   1 :   33554432 : layers.0.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   33554432 : layers.0.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   5 :  109051904 : layers.0.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   6 :  109051904 : layers.0.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   7 :  109051904 : layers.0.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   33554432 : layers.1.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   33554432 : layers.1.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  14 :  109051904 : layers.1.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  15 :  109051904 : layers.1.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  16 :  109051904 : layers.1.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   33554432 : layers.2.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   33554432 : layers.2.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  23 :  109051904 : layers.2.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  24 :  109051904 : layers.2.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  25 :  109051904 : layers.2.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   33554432 : layers.3.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   33554432 : layers.3.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  32 :  109051904 : layers.3.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  33 :  109051904 : layers.3.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  34 :  109051904 : layers.3.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   33554432 : layers.4.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   33554432 : layers.4.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  41 :  109051904 : layers.4.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  42 :  109051904 : layers.4.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  43 :  109051904 : layers.4.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   33554432 : layers.5.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   33554432 : layers.5.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  50 :  109051904 : layers.5.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
205376684032 params in total.
410753368064 bytes in total.
3903.83 sec, 48.81 sec, 100.34 MB/s
-----------------------------------------------------------------------------

[90/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00002-of-00022.pth
   0 :  109051904 : layers.5.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.5.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
   3 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
   4 :   33554432 : layers.6.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.6.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.6.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.6.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.6.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  12 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  13 :   33554432 : layers.7.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.7.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.7.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.7.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.7.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  21 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  22 :   33554432 : layers.8.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.8.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.8.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.8.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.8.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  30 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  31 :   33554432 : layers.9.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.9.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.9.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.9.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.9.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  39 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  40 :   33554432 : layers.10.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.10.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.10.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.10.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.10.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.11.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.11.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.11.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
207767633920 params in total.
415535267840 bytes in total.
3952.45 sec, 48.62 sec, 100.26 MB/s
-----------------------------------------------------------------------------

[91/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00003-of-00022.pth
   0 :  109051904 : layers.11.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.11.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.12.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.12.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.12.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.12.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.12.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.13.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.13.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.13.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.13.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.13.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.14.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.14.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.14.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.14.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.14.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.15.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.15.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.15.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.15.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.15.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.16.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.16.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.16.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.16.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.16.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.17.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.17.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.17.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
210158583808 params in total.
420317167616 bytes in total.
4000.85 sec, 48.39 sec, 100.19 MB/s
-----------------------------------------------------------------------------

[92/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00004-of-00022.pth
   0 :  109051904 : layers.17.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.17.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.18.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.18.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.18.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.18.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.18.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.19.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.19.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.19.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.19.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.19.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.20.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.20.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.20.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.20.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.20.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.21.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.21.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.21.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.21.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.21.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.22.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.22.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.22.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.22.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.22.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.23.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.23.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.23.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
212549533696 params in total.
425099067392 bytes in total.
4049.77 sec, 48.92 sec, 100.11 MB/s
-----------------------------------------------------------------------------

[93/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00005-of-00022.pth
   0 :  109051904 : layers.23.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.23.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.24.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.24.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.24.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.24.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.24.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.25.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.25.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.25.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.25.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.25.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.26.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.26.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.26.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.26.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.26.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.27.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.27.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.27.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.27.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.27.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.28.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.28.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.28.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.28.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.28.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.29.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.29.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.29.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
214940483584 params in total.
429880967168 bytes in total.
4098.54 sec, 48.78 sec, 100.03 MB/s
-----------------------------------------------------------------------------

[94/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00006-of-00022.pth
   0 :  109051904 : layers.29.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.29.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.30.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.30.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.30.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.30.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.30.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.31.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.31.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.31.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.31.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.31.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.32.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.32.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.32.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.32.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.32.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.33.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.33.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.33.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.33.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.33.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.34.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.34.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.34.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.34.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.34.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.35.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.35.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.35.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
217331433472 params in total.
434662866944 bytes in total.
4147.16 sec, 48.62 sec, 99.95 MB/s
-----------------------------------------------------------------------------

[95/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00007-of-00022.pth
   0 :  109051904 : layers.35.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.35.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.36.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.36.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.36.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.36.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.36.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.37.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.37.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.37.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.37.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.37.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.38.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.38.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.38.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.38.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.38.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.39.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.39.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.39.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.39.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.39.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.40.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.40.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.40.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.40.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.40.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.41.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.41.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.41.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
219722383360 params in total.
439444766720 bytes in total.
4195.74 sec, 48.58 sec, 99.88 MB/s
-----------------------------------------------------------------------------

[96/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00008-of-00022.pth
   0 :  109051904 : layers.41.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.41.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.42.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.42.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.42.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.42.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.42.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.43.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.43.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.43.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.43.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.43.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.44.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.44.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.44.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.44.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.44.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.45.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.45.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.45.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.45.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.45.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.46.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.46.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.46.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.46.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.46.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.47.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.47.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.47.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
222113333248 params in total.
444226666496 bytes in total.
4244.64 sec, 48.90 sec, 99.81 MB/s
-----------------------------------------------------------------------------

[97/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00009-of-00022.pth
   0 :  109051904 : layers.47.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.47.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.48.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.48.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.48.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.48.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.48.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.49.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.49.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.49.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.49.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.49.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.50.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.50.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.50.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.50.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.50.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.51.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.51.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.51.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.51.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.51.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.52.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.52.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.52.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.52.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.52.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.53.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.53.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.53.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
224504283136 params in total.
449008566272 bytes in total.
4293.26 sec, 48.62 sec, 99.74 MB/s
-----------------------------------------------------------------------------

[98/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00010-of-00022.pth
   0 :  109051904 : layers.53.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.53.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.54.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.54.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.54.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.54.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.54.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.55.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.55.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.55.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.55.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.55.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.56.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.56.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.56.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.56.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.56.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.57.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.57.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.57.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.57.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.57.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.58.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.58.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.58.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.58.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.58.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.59.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.59.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.59.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
226895233024 params in total.
453790466048 bytes in total.
4341.93 sec, 48.66 sec, 99.67 MB/s
-----------------------------------------------------------------------------

[99/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00011-of-00022.pth
   0 :  109051904 : layers.59.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.59.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.60.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.60.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.60.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.60.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.60.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.61.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.61.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.61.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.61.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.61.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.62.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.62.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.62.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.62.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.62.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.63.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.63.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.63.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.63.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.63.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.64.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.64.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.64.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.64.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.64.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.65.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.65.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.65.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
229286182912 params in total.
458572365824 bytes in total.
4390.55 sec, 48.63 sec, 99.61 MB/s
-----------------------------------------------------------------------------

[100/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00012-of-00022.pth
   0 :  109051904 : layers.65.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.65.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.66.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.66.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.66.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.66.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.66.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.67.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.67.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.67.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.67.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.67.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.68.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.68.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.68.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.68.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.68.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.69.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.69.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.69.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.69.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.69.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.70.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.70.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.70.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.70.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.70.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.71.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.71.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.71.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
231677132800 params in total.
463354265600 bytes in total.
4439.41 sec, 48.86 sec, 99.54 MB/s
-----------------------------------------------------------------------------

[101/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00013-of-00022.pth
   0 :  109051904 : layers.71.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.71.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.72.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.72.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.72.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.72.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.72.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.73.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.73.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.73.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.73.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.73.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.74.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.74.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.74.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.74.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.74.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.75.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.75.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.75.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.75.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.75.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.76.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.76.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.76.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.76.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.76.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.77.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.77.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.77.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
234068082688 params in total.
468136165376 bytes in total.
4488.38 sec, 48.97 sec, 99.47 MB/s
-----------------------------------------------------------------------------

[102/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00014-of-00022.pth
   0 :  109051904 : layers.77.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.77.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.78.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.78.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.78.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.78.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.78.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.79.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.79.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.79.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.79.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.79.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.80.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.80.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.80.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.80.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.80.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.81.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.81.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.81.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.81.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.81.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.82.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.82.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.82.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.82.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.82.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.83.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.83.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.83.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
236459032576 params in total.
472918065152 bytes in total.
4537.38 sec, 49.00 sec, 99.40 MB/s
-----------------------------------------------------------------------------

[103/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00015-of-00022.pth
   0 :  109051904 : layers.83.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.83.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.84.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.84.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.84.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.84.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.84.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.85.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.85.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.85.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.85.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.85.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.86.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.86.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.86.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.86.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.86.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.87.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.87.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.87.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.87.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.87.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.88.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.88.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.88.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.88.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.88.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.89.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.89.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.89.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
238849982464 params in total.
477699964928 bytes in total.
4587.23 sec, 49.85 sec, 99.31 MB/s
-----------------------------------------------------------------------------

[104/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00016-of-00022.pth
   0 :  109051904 : layers.89.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.89.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.90.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.90.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.90.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.90.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.90.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.91.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.91.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.91.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.91.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.91.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.92.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.92.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.92.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.92.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.92.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.93.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.93.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.93.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.93.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.93.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.94.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.94.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.94.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.94.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.94.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.95.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.95.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.95.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
241240932352 params in total.
482481864704 bytes in total.
4636.74 sec, 49.51 sec, 99.24 MB/s
-----------------------------------------------------------------------------

[105/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00017-of-00022.pth
   0 :  109051904 : layers.95.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.95.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.96.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.96.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.96.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.96.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.96.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.97.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.97.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.97.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.97.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.97.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.98.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.98.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.98.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.98.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.98.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.99.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.99.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.99.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.99.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.99.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.100.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.100.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.100.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.100.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.100.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.101.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.101.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.101.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
243631882240 params in total.
487263764480 bytes in total.
4686.14 sec, 49.40 sec, 99.16 MB/s
-----------------------------------------------------------------------------

[106/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00018-of-00022.pth
   0 :  109051904 : layers.101.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.101.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.102.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.102.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.102.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.102.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.102.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.103.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.103.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.103.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.103.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.103.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.104.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.104.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.104.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.104.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.104.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.105.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.105.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.105.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.105.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.105.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.106.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.106.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.106.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.106.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.106.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.107.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.107.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.107.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
246022832128 params in total.
492045664256 bytes in total.
4735.53 sec, 49.39 sec, 99.09 MB/s
-----------------------------------------------------------------------------

[107/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00019-of-00022.pth
   0 :  109051904 : layers.107.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.107.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.108.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.108.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.108.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.108.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.108.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.109.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.109.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.109.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.109.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.109.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.110.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.110.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.110.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.110.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.110.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.111.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.111.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.111.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.111.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.111.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.112.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.112.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.112.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.112.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.112.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.113.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.113.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.113.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
248413782016 params in total.
496827564032 bytes in total.
4785.22 sec, 49.69 sec, 99.02 MB/s
-----------------------------------------------------------------------------

[108/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00020-of-00022.pth
   0 :  109051904 : layers.113.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.113.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.114.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.114.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.114.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.114.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.114.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.115.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.115.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.115.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.115.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.115.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.116.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.116.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.116.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.116.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.116.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.117.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.117.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.117.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.117.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.117.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.118.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.118.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.118.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.118.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.118.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.119.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.119.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.119.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
250804731904 params in total.
501609463808 bytes in total.
4834.88 sec, 49.66 sec, 98.94 MB/s
-----------------------------------------------------------------------------

[109/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00021-of-00022.pth
   0 :  109051904 : layers.119.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.119.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.120.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.120.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.120.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.120.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.120.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.121.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.121.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.121.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.121.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.121.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.122.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.122.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.122.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.122.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.122.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.123.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.123.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.123.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.123.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.123.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.124.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.124.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.124.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.124.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.124.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.125.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.125.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.125.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
253195681792 params in total.
506391363584 bytes in total.
4884.48 sec, 49.60 sec, 98.87 MB/s
-----------------------------------------------------------------------------

[110/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.04/consolidated-00022-of-00022.pth
   0 :  109051904 : layers.125.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.125.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
   5 :  262668288 : output.weight                       : [16032, 16384]  : torch.bfloat16

-----------------------------------------------------------------------------
253676503040 params in total.
507353006080 bytes in total.
4894.80 sec, 10.31 sec, 98.85 MB/s
-----------------------------------------------------------------------------

[111/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00001-of-00022.pth
   0 :  262668288 : tok_embeddings.weight               : [16032, 16384]  : torch.bfloat16
   1 :   33554432 : layers.0.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   33554432 : layers.0.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   5 :  109051904 : layers.0.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   6 :  109051904 : layers.0.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   7 :  109051904 : layers.0.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   33554432 : layers.1.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   33554432 : layers.1.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  14 :  109051904 : layers.1.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  15 :  109051904 : layers.1.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  16 :  109051904 : layers.1.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   33554432 : layers.2.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   33554432 : layers.2.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  23 :  109051904 : layers.2.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  24 :  109051904 : layers.2.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  25 :  109051904 : layers.2.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   33554432 : layers.3.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   33554432 : layers.3.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  32 :  109051904 : layers.3.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  33 :  109051904 : layers.3.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  34 :  109051904 : layers.3.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   33554432 : layers.4.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   33554432 : layers.4.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  41 :  109051904 : layers.4.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  42 :  109051904 : layers.4.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  43 :  109051904 : layers.4.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   33554432 : layers.5.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   33554432 : layers.5.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  50 :  109051904 : layers.5.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
256111984640 params in total.
512223969280 bytes in total.
4944.95 sec, 50.15 sec, 98.79 MB/s
-----------------------------------------------------------------------------

[112/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00002-of-00022.pth
   0 :  109051904 : layers.5.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.5.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
   3 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
   4 :   33554432 : layers.6.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.6.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.6.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.6.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.6.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  12 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  13 :   33554432 : layers.7.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.7.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.7.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.7.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.7.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  21 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  22 :   33554432 : layers.8.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.8.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.8.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.8.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.8.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  30 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  31 :   33554432 : layers.9.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.9.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.9.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.9.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.9.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  39 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  40 :   33554432 : layers.10.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.10.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.10.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.10.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.10.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.11.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.11.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.11.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
258502934528 params in total.
517005869056 bytes in total.
4994.41 sec, 49.46 sec, 98.72 MB/s
-----------------------------------------------------------------------------

[113/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00003-of-00022.pth
   0 :  109051904 : layers.11.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.11.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.12.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.12.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.12.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.12.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.12.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.13.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.13.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.13.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.13.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.13.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.14.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.14.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.14.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.14.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.14.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.15.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.15.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.15.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.15.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.15.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.16.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.16.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.16.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.16.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.16.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.17.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.17.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.17.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
260893884416 params in total.
521787768832 bytes in total.
5044.27 sec, 49.87 sec, 98.65 MB/s
-----------------------------------------------------------------------------

[114/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00004-of-00022.pth
   0 :  109051904 : layers.17.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.17.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.18.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.18.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.18.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.18.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.18.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.19.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.19.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.19.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.19.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.19.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.20.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.20.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.20.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.20.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.20.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.21.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.21.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.21.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.21.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.21.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.22.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.22.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.22.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.22.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.22.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.23.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.23.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.23.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
263284834304 params in total.
526569668608 bytes in total.
5094.06 sec, 49.79 sec, 98.58 MB/s
-----------------------------------------------------------------------------

[115/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00005-of-00022.pth
   0 :  109051904 : layers.23.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.23.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.24.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.24.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.24.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.24.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.24.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.25.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.25.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.25.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.25.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.25.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.26.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.26.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.26.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.26.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.26.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.27.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.27.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.27.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.27.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.27.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.28.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.28.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.28.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.28.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.28.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.29.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.29.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.29.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
265675784192 params in total.
531351568384 bytes in total.
5143.70 sec, 49.64 sec, 98.52 MB/s
-----------------------------------------------------------------------------

[116/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00006-of-00022.pth
   0 :  109051904 : layers.29.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.29.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.30.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.30.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.30.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.30.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.30.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.31.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.31.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.31.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.31.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.31.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.32.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.32.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.32.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.32.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.32.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.33.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.33.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.33.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.33.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.33.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.34.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.34.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.34.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.34.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.34.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.35.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.35.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.35.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
268066734080 params in total.
536133468160 bytes in total.
5193.45 sec, 49.75 sec, 98.45 MB/s
-----------------------------------------------------------------------------

[117/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00007-of-00022.pth
   0 :  109051904 : layers.35.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.35.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.36.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.36.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.36.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.36.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.36.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.37.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.37.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.37.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.37.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.37.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.38.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.38.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.38.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.38.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.38.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.39.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.39.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.39.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.39.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.39.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.40.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.40.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.40.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.40.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.40.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.41.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.41.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.41.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
270457683968 params in total.
540915367936 bytes in total.
5243.36 sec, 49.91 sec, 98.38 MB/s
-----------------------------------------------------------------------------

[118/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00008-of-00022.pth
   0 :  109051904 : layers.41.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.41.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.42.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.42.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.42.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.42.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.42.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.43.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.43.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.43.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.43.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.43.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.44.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.44.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.44.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.44.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.44.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.45.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.45.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.45.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.45.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.45.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.46.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.46.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.46.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.46.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.46.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.47.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.47.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.47.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
272848633856 params in total.
545697267712 bytes in total.
5293.17 sec, 49.80 sec, 98.32 MB/s
-----------------------------------------------------------------------------

[119/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00009-of-00022.pth
   0 :  109051904 : layers.47.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.47.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.48.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.48.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.48.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.48.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.48.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.49.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.49.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.49.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.49.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.49.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.50.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.50.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.50.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.50.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.50.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.51.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.51.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.51.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.51.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.51.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.52.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.52.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.52.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.52.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.52.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.53.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.53.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.53.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
275239583744 params in total.
550479167488 bytes in total.
5343.11 sec, 49.95 sec, 98.25 MB/s
-----------------------------------------------------------------------------

[120/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00010-of-00022.pth
   0 :  109051904 : layers.53.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.53.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.54.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.54.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.54.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.54.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.54.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.55.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.55.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.55.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.55.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.55.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.56.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.56.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.56.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.56.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.56.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.57.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.57.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.57.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.57.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.57.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.58.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.58.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.58.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.58.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.58.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.59.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.59.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.59.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
277630533632 params in total.
555261067264 bytes in total.
5393.15 sec, 50.04 sec, 98.19 MB/s
-----------------------------------------------------------------------------

[121/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00011-of-00022.pth
   0 :  109051904 : layers.59.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.59.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.60.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.60.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.60.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.60.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.60.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.61.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.61.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.61.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.61.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.61.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.62.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.62.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.62.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.62.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.62.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.63.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.63.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.63.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.63.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.63.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.64.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.64.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.64.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.64.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.64.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.65.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.65.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.65.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
280021483520 params in total.
560042967040 bytes in total.
5443.93 sec, 50.78 sec, 98.11 MB/s
-----------------------------------------------------------------------------

[122/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00012-of-00022.pth
   0 :  109051904 : layers.65.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.65.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.66.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.66.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.66.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.66.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.66.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.67.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.67.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.67.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.67.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.67.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.68.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.68.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.68.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.68.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.68.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.69.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.69.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.69.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.69.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.69.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.70.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.70.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.70.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.70.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.70.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.71.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.71.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.71.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
282412433408 params in total.
564824866816 bytes in total.
5494.63 sec, 50.69 sec, 98.03 MB/s
-----------------------------------------------------------------------------

[123/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00013-of-00022.pth
   0 :  109051904 : layers.71.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.71.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.72.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.72.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.72.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.72.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.72.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.73.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.73.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.73.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.73.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.73.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.74.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.74.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.74.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.74.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.74.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.75.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.75.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.75.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.75.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.75.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.76.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.76.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.76.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.76.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.76.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.77.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.77.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.77.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
284803383296 params in total.
569606766592 bytes in total.
5545.49 sec, 50.86 sec, 97.96 MB/s
-----------------------------------------------------------------------------

[124/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00014-of-00022.pth
   0 :  109051904 : layers.77.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.77.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.78.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.78.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.78.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.78.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.78.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.79.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.79.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.79.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.79.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.79.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.80.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.80.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.80.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.80.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.80.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.81.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.81.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.81.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.81.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.81.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.82.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.82.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.82.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.82.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.82.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.83.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.83.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.83.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
287194333184 params in total.
574388666368 bytes in total.
5596.41 sec, 50.92 sec, 97.88 MB/s
-----------------------------------------------------------------------------

[125/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00015-of-00022.pth
   0 :  109051904 : layers.83.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.83.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.84.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.84.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.84.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.84.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.84.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.85.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.85.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.85.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.85.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.85.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.86.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.86.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.86.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.86.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.86.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.87.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.87.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.87.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.87.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.87.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.88.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.88.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.88.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.88.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.88.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.89.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.89.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.89.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
289585283072 params in total.
579170566144 bytes in total.
5646.92 sec, 50.51 sec, 97.81 MB/s
-----------------------------------------------------------------------------

[126/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00016-of-00022.pth
   0 :  109051904 : layers.89.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.89.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.90.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.90.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.90.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.90.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.90.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.91.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.91.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.91.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.91.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.91.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.92.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.92.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.92.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.92.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.92.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.93.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.93.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.93.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.93.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.93.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.94.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.94.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.94.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.94.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.94.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.95.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.95.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.95.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
291976232960 params in total.
583952465920 bytes in total.
5697.53 sec, 50.61 sec, 97.74 MB/s
-----------------------------------------------------------------------------

[127/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00017-of-00022.pth
   0 :  109051904 : layers.95.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.95.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.96.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.96.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.96.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.96.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.96.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.97.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.97.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.97.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.97.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.97.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.98.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.98.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.98.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.98.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.98.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.99.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.99.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.99.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.99.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.99.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.100.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.100.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.100.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.100.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.100.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.101.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.101.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.101.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
294367182848 params in total.
588734365696 bytes in total.
5748.25 sec, 50.73 sec, 97.68 MB/s
-----------------------------------------------------------------------------

[128/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00018-of-00022.pth
   0 :  109051904 : layers.101.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.101.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.102.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.102.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.102.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.102.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.102.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.103.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.103.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.103.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.103.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.103.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.104.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.104.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.104.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.104.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.104.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.105.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.105.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.105.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.105.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.105.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.106.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.106.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.106.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.106.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.106.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.107.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.107.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.107.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
296758132736 params in total.
593516265472 bytes in total.
5799.09 sec, 50.83 sec, 97.61 MB/s
-----------------------------------------------------------------------------

[129/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00019-of-00022.pth
   0 :  109051904 : layers.107.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.107.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.108.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.108.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.108.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.108.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.108.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.109.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.109.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.109.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.109.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.109.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.110.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.110.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.110.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.110.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.110.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.111.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.111.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.111.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.111.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.111.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.112.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.112.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.112.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.112.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.112.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.113.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.113.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.113.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
299149082624 params in total.
598298165248 bytes in total.
5850.00 sec, 50.91 sec, 97.54 MB/s
-----------------------------------------------------------------------------

[130/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00020-of-00022.pth
   0 :  109051904 : layers.113.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.113.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.114.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.114.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.114.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.114.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.114.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.115.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.115.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.115.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.115.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.115.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.116.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.116.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.116.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.116.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.116.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.117.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.117.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.117.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.117.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.117.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.118.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.118.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.118.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.118.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.118.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.119.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.119.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.119.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
301540032512 params in total.
603080065024 bytes in total.
5901.47 sec, 51.48 sec, 97.46 MB/s
-----------------------------------------------------------------------------

[131/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00021-of-00022.pth
   0 :  109051904 : layers.119.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.119.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.120.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.120.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.120.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.120.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.120.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.121.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.121.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.121.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.121.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.121.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.122.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.122.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.122.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.122.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.122.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.123.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.123.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.123.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.123.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.123.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.124.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.124.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.124.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.124.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.124.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.125.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.125.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.125.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
303930982400 params in total.
607861964800 bytes in total.
5952.71 sec, 51.24 sec, 97.38 MB/s
-----------------------------------------------------------------------------

[132/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.05/consolidated-00022-of-00022.pth
   0 :  109051904 : layers.125.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.125.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
   5 :  262668288 : output.weight                       : [16032, 16384]  : torch.bfloat16

-----------------------------------------------------------------------------
304411803648 params in total.
608823607296 bytes in total.
5963.78 sec, 11.06 sec, 97.36 MB/s
-----------------------------------------------------------------------------

[133/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00001-of-00022.pth
   0 :  262668288 : tok_embeddings.weight               : [16032, 16384]  : torch.bfloat16
   1 :   33554432 : layers.0.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   33554432 : layers.0.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   5 :  109051904 : layers.0.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   6 :  109051904 : layers.0.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   7 :  109051904 : layers.0.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   33554432 : layers.1.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   33554432 : layers.1.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  14 :  109051904 : layers.1.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  15 :  109051904 : layers.1.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  16 :  109051904 : layers.1.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   33554432 : layers.2.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   33554432 : layers.2.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  23 :  109051904 : layers.2.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  24 :  109051904 : layers.2.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  25 :  109051904 : layers.2.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   33554432 : layers.3.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   33554432 : layers.3.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  32 :  109051904 : layers.3.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  33 :  109051904 : layers.3.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  34 :  109051904 : layers.3.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   33554432 : layers.4.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   33554432 : layers.4.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  41 :  109051904 : layers.4.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  42 :  109051904 : layers.4.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  43 :  109051904 : layers.4.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   33554432 : layers.5.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   33554432 : layers.5.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  50 :  109051904 : layers.5.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
306847285248 params in total.
613694570496 bytes in total.
6015.63 sec, 51.85 sec, 97.29 MB/s
-----------------------------------------------------------------------------

[134/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00002-of-00022.pth
   0 :  109051904 : layers.5.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.5.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
   3 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
   4 :   33554432 : layers.6.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.6.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.6.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.6.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.6.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  12 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  13 :   33554432 : layers.7.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.7.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.7.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.7.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.7.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  21 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  22 :   33554432 : layers.8.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.8.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.8.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.8.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.8.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  30 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  31 :   33554432 : layers.9.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.9.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.9.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.9.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.9.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  39 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  40 :   33554432 : layers.10.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.10.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.10.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.10.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.10.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.11.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.11.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.11.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
309238235136 params in total.
618476470272 bytes in total.
6066.72 sec, 51.09 sec, 97.22 MB/s
-----------------------------------------------------------------------------

[135/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00003-of-00022.pth
   0 :  109051904 : layers.11.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.11.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.12.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.12.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.12.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.12.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.12.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.13.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.13.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.13.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.13.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.13.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.14.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.14.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.14.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.14.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.14.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.15.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.15.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.15.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.15.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.15.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.16.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.16.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.16.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.16.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.16.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.17.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.17.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.17.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
311629185024 params in total.
623258370048 bytes in total.
6118.24 sec, 51.52 sec, 97.15 MB/s
-----------------------------------------------------------------------------

[136/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00004-of-00022.pth
   0 :  109051904 : layers.17.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.17.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.18.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.18.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.18.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.18.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.18.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.19.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.19.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.19.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.19.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.19.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.20.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.20.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.20.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.20.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.20.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.21.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.21.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.21.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.21.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.21.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.22.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.22.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.22.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.22.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.22.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.23.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.23.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.23.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
314020134912 params in total.
628040269824 bytes in total.
6169.44 sec, 51.20 sec, 97.08 MB/s
-----------------------------------------------------------------------------

[137/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00005-of-00022.pth
   0 :  109051904 : layers.23.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.23.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.24.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.24.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.24.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.24.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.24.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.25.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.25.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.25.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.25.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.25.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.26.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.26.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.26.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.26.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.26.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.27.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.27.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.27.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.27.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.27.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.28.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.28.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.28.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.28.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.28.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.29.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.29.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.29.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
316411084800 params in total.
632822169600 bytes in total.
6221.06 sec, 51.62 sec, 97.01 MB/s
-----------------------------------------------------------------------------

[138/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00006-of-00022.pth
   0 :  109051904 : layers.29.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.29.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.30.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.30.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.30.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.30.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.30.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.31.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.31.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.31.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.31.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.31.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.32.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.32.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.32.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.32.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.32.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.33.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.33.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.33.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.33.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.33.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.34.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.34.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.34.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.34.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.34.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.35.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.35.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.35.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
318802034688 params in total.
637604069376 bytes in total.
6273.02 sec, 51.96 sec, 96.93 MB/s
-----------------------------------------------------------------------------

[139/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00007-of-00022.pth
   0 :  109051904 : layers.35.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.35.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.36.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.36.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.36.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.36.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.36.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.37.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.37.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.37.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.37.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.37.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.38.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.38.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.38.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.38.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.38.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.39.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.39.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.39.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.39.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.39.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.40.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.40.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.40.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.40.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.40.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.41.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.41.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.41.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
321192984576 params in total.
642385969152 bytes in total.
6324.73 sec, 51.71 sec, 96.86 MB/s
-----------------------------------------------------------------------------

[140/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00008-of-00022.pth
   0 :  109051904 : layers.41.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.41.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.42.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.42.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.42.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.42.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.42.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.43.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.43.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.43.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.43.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.43.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.44.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.44.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.44.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.44.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.44.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.45.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.45.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.45.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.45.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.45.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.46.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.46.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.46.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.46.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.46.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.47.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.47.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.47.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
323583934464 params in total.
647167868928 bytes in total.
6376.50 sec, 51.76 sec, 96.79 MB/s
-----------------------------------------------------------------------------

[141/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00009-of-00022.pth
   0 :  109051904 : layers.47.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.47.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.48.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.48.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.48.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.48.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.48.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.49.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.49.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.49.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.49.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.49.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.50.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.50.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.50.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.50.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.50.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.51.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.51.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.51.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.51.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.51.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.52.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.52.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.52.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.52.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.52.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.53.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.53.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.53.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
325974884352 params in total.
651949768704 bytes in total.
6428.46 sec, 51.96 sec, 96.72 MB/s
-----------------------------------------------------------------------------

[142/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00010-of-00022.pth
   0 :  109051904 : layers.53.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.53.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.54.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.54.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.54.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.54.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.54.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.55.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.55.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.55.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.55.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.55.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.56.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.56.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.56.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.56.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.56.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.57.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.57.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.57.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.57.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.57.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.58.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.58.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.58.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.58.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.58.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.59.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.59.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.59.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
328365834240 params in total.
656731668480 bytes in total.
6480.65 sec, 52.19 sec, 96.64 MB/s
-----------------------------------------------------------------------------

[143/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00011-of-00022.pth
   0 :  109051904 : layers.59.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.59.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.60.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.60.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.60.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.60.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.60.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.61.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.61.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.61.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.61.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.61.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.62.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.62.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.62.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.62.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.62.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.63.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.63.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.63.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.63.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.63.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.64.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.64.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.64.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.64.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.64.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.65.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.65.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.65.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
330756784128 params in total.
661513568256 bytes in total.
6532.67 sec, 52.02 sec, 96.57 MB/s
-----------------------------------------------------------------------------

[144/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00012-of-00022.pth
   0 :  109051904 : layers.65.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.65.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.66.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.66.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.66.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.66.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.66.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.67.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.67.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.67.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.67.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.67.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.68.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.68.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.68.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.68.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.68.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.69.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.69.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.69.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.69.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.69.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.70.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.70.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.70.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.70.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.70.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.71.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.71.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.71.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
333147734016 params in total.
666295468032 bytes in total.
6584.54 sec, 51.87 sec, 96.50 MB/s
-----------------------------------------------------------------------------

[145/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00013-of-00022.pth
   0 :  109051904 : layers.71.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.71.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.72.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.72.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.72.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.72.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.72.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.73.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.73.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.73.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.73.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.73.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.74.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.74.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.74.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.74.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.74.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.75.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.75.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.75.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.75.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.75.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.76.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.76.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.76.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.76.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.76.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.77.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.77.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.77.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
335538683904 params in total.
671077367808 bytes in total.
6636.77 sec, 52.23 sec, 96.43 MB/s
-----------------------------------------------------------------------------

[146/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00014-of-00022.pth
   0 :  109051904 : layers.77.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.77.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.78.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.78.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.78.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.78.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.78.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.79.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.79.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.79.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.79.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.79.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.80.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.80.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.80.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.80.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.80.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.81.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.81.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.81.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.81.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.81.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.82.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.82.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.82.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.82.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.82.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.83.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.83.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.83.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
337929633792 params in total.
675859267584 bytes in total.
6689.26 sec, 52.50 sec, 96.36 MB/s
-----------------------------------------------------------------------------

[147/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00015-of-00022.pth
   0 :  109051904 : layers.83.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.83.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.84.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.84.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.84.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.84.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.84.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.85.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.85.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.85.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.85.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.85.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.86.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.86.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.86.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.86.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.86.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.87.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.87.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.87.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.87.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.87.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.88.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.88.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.88.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.88.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.88.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.89.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.89.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.89.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
340320583680 params in total.
680641167360 bytes in total.
6741.56 sec, 52.30 sec, 96.28 MB/s
-----------------------------------------------------------------------------

[148/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00016-of-00022.pth
   0 :  109051904 : layers.89.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.89.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.90.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.90.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.90.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.90.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.90.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.91.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.91.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.91.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.91.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.91.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.92.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.92.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.92.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.92.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.92.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.93.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.93.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.93.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.93.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.93.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.94.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.94.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.94.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.94.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.94.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.95.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.95.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.95.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
342711533568 params in total.
685423067136 bytes in total.
6794.19 sec, 52.63 sec, 96.21 MB/s
-----------------------------------------------------------------------------

[149/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00017-of-00022.pth
   0 :  109051904 : layers.95.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.95.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.96.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.96.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.96.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.96.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.96.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.97.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.97.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.97.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.97.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.97.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.98.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.98.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.98.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.98.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.98.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.99.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.99.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.99.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.99.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.99.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.100.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.100.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.100.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.100.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.100.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.101.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.101.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.101.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
345102483456 params in total.
690204966912 bytes in total.
6846.94 sec, 52.75 sec, 96.14 MB/s
-----------------------------------------------------------------------------

[150/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00018-of-00022.pth
   0 :  109051904 : layers.101.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.101.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.102.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.102.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.102.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.102.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.102.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.103.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.103.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.103.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.103.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.103.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.104.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.104.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.104.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.104.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.104.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.105.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.105.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.105.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.105.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.105.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.106.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.106.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.106.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.106.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.106.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.107.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.107.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.107.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
347493433344 params in total.
694986866688 bytes in total.
6899.69 sec, 52.75 sec, 96.06 MB/s
-----------------------------------------------------------------------------

[151/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00019-of-00022.pth
   0 :  109051904 : layers.107.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.107.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.108.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.108.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.108.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.108.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.108.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.109.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.109.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.109.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.109.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.109.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.110.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.110.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.110.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.110.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.110.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.111.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.111.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.111.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.111.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.111.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.112.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.112.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.112.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.112.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.112.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.113.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.113.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.113.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
349884383232 params in total.
699768766464 bytes in total.
6952.03 sec, 52.34 sec, 95.99 MB/s
-----------------------------------------------------------------------------

[152/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00020-of-00022.pth
   0 :  109051904 : layers.113.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.113.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.114.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.114.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.114.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.114.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.114.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.115.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.115.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.115.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.115.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.115.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.116.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.116.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.116.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.116.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.116.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.117.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.117.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.117.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.117.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.117.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.118.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.118.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.118.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.118.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.118.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.119.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.119.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.119.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
352275333120 params in total.
704550666240 bytes in total.
7004.33 sec, 52.30 sec, 95.93 MB/s
-----------------------------------------------------------------------------

[153/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00021-of-00022.pth
   0 :  109051904 : layers.119.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.119.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.120.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.120.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.120.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.120.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.120.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.121.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.121.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.121.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.121.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.121.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.122.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.122.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.122.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.122.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.122.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.123.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.123.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.123.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.123.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.123.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.124.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.124.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.124.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.124.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.124.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.125.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.125.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.125.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
354666283008 params in total.
709332566016 bytes in total.
7057.11 sec, 52.78 sec, 95.86 MB/s
-----------------------------------------------------------------------------

[154/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.06/consolidated-00022-of-00022.pth
   0 :  109051904 : layers.125.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.125.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
   5 :  262668288 : output.weight                       : [16032, 16384]  : torch.bfloat16

-----------------------------------------------------------------------------
355147104256 params in total.
710294208512 bytes in total.
7068.35 sec, 11.24 sec, 95.83 MB/s
-----------------------------------------------------------------------------

[155/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00001-of-00022.pth
   0 :  262668288 : tok_embeddings.weight               : [16032, 16384]  : torch.bfloat16
   1 :   33554432 : layers.0.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   2 :    2097152 : layers.0.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   3 :    2097152 : layers.0.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   4 :   33554432 : layers.0.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   5 :  109051904 : layers.0.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   6 :  109051904 : layers.0.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   7 :  109051904 : layers.0.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   8 :      16384 : layers.0.attention_norm.weight      : [16384]         : torch.bfloat16
   9 :      16384 : layers.0.ffn_norm.weight            : [16384]         : torch.bfloat16
  10 :   33554432 : layers.1.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  11 :    2097152 : layers.1.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  12 :    2097152 : layers.1.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  13 :   33554432 : layers.1.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  14 :  109051904 : layers.1.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  15 :  109051904 : layers.1.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  16 :  109051904 : layers.1.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  17 :      16384 : layers.1.attention_norm.weight      : [16384]         : torch.bfloat16
  18 :      16384 : layers.1.ffn_norm.weight            : [16384]         : torch.bfloat16
  19 :   33554432 : layers.2.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  20 :    2097152 : layers.2.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  21 :    2097152 : layers.2.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  22 :   33554432 : layers.2.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  23 :  109051904 : layers.2.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  24 :  109051904 : layers.2.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  25 :  109051904 : layers.2.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  26 :      16384 : layers.2.attention_norm.weight      : [16384]         : torch.bfloat16
  27 :      16384 : layers.2.ffn_norm.weight            : [16384]         : torch.bfloat16
  28 :   33554432 : layers.3.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  29 :    2097152 : layers.3.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  30 :    2097152 : layers.3.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  31 :   33554432 : layers.3.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  32 :  109051904 : layers.3.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  33 :  109051904 : layers.3.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  34 :  109051904 : layers.3.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  35 :      16384 : layers.3.attention_norm.weight      : [16384]         : torch.bfloat16
  36 :      16384 : layers.3.ffn_norm.weight            : [16384]         : torch.bfloat16
  37 :   33554432 : layers.4.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  38 :    2097152 : layers.4.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  39 :    2097152 : layers.4.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  40 :   33554432 : layers.4.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  41 :  109051904 : layers.4.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  42 :  109051904 : layers.4.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  43 :  109051904 : layers.4.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  44 :      16384 : layers.4.attention_norm.weight      : [16384]         : torch.bfloat16
  45 :      16384 : layers.4.ffn_norm.weight            : [16384]         : torch.bfloat16
  46 :   33554432 : layers.5.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  47 :    2097152 : layers.5.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  48 :    2097152 : layers.5.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  49 :   33554432 : layers.5.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  50 :  109051904 : layers.5.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
357582585856 params in total.
715165171712 bytes in total.
7122.53 sec, 54.17 sec, 95.76 MB/s
-----------------------------------------------------------------------------

[156/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00002-of-00022.pth
   0 :  109051904 : layers.5.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.5.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.5.attention_norm.weight      : [16384]         : torch.bfloat16
   3 :      16384 : layers.5.ffn_norm.weight            : [16384]         : torch.bfloat16
   4 :   33554432 : layers.6.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.6.attention.wk.weight        : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.6.attention.wv.weight        : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.6.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.6.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.6.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.6.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.6.attention_norm.weight      : [16384]         : torch.bfloat16
  12 :      16384 : layers.6.ffn_norm.weight            : [16384]         : torch.bfloat16
  13 :   33554432 : layers.7.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.7.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.7.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.7.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.7.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.7.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.7.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.7.attention_norm.weight      : [16384]         : torch.bfloat16
  21 :      16384 : layers.7.ffn_norm.weight            : [16384]         : torch.bfloat16
  22 :   33554432 : layers.8.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.8.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.8.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.8.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.8.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.8.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.8.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.8.attention_norm.weight      : [16384]         : torch.bfloat16
  30 :      16384 : layers.8.ffn_norm.weight            : [16384]         : torch.bfloat16
  31 :   33554432 : layers.9.attention.wq.weight        : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.9.attention.wk.weight        : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.9.attention.wv.weight        : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.9.attention.wo.weight        : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.9.feed_forward.w1.weight     : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.9.feed_forward.w3.weight     : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.9.feed_forward.w2.weight     : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.9.attention_norm.weight      : [16384]         : torch.bfloat16
  39 :      16384 : layers.9.ffn_norm.weight            : [16384]         : torch.bfloat16
  40 :   33554432 : layers.10.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.10.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.10.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.10.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.10.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.10.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.10.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.10.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.10.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.11.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.11.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.11.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.11.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.11.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
359973535744 params in total.
719947071488 bytes in total.
7175.69 sec, 53.16 sec, 95.68 MB/s
-----------------------------------------------------------------------------

[157/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00003-of-00022.pth
   0 :  109051904 : layers.11.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.11.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.11.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.11.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.12.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.12.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.12.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.12.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.12.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.12.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.12.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.12.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.12.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.13.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.13.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.13.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.13.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.13.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.13.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.13.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.13.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.13.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.14.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.14.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.14.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.14.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.14.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.14.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.14.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.14.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.14.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.15.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.15.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.15.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.15.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.15.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.15.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.15.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.15.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.15.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.16.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.16.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.16.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.16.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.16.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.16.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.16.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.16.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.16.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.17.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.17.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.17.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.17.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.17.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
362364485632 params in total.
724728971264 bytes in total.
7228.95 sec, 53.26 sec, 95.61 MB/s
-----------------------------------------------------------------------------

[158/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00004-of-00022.pth
   0 :  109051904 : layers.17.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.17.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.17.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.17.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.18.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.18.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.18.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.18.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.18.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.18.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.18.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.18.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.18.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.19.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.19.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.19.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.19.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.19.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.19.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.19.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.19.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.19.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.20.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.20.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.20.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.20.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.20.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.20.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.20.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.20.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.20.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.21.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.21.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.21.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.21.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.21.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.21.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.21.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.21.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.21.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.22.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.22.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.22.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.22.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.22.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.22.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.22.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.22.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.22.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.23.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.23.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.23.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.23.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.23.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
364755435520 params in total.
729510871040 bytes in total.
7282.29 sec, 53.34 sec, 95.54 MB/s
-----------------------------------------------------------------------------

[159/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00005-of-00022.pth
   0 :  109051904 : layers.23.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.23.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.23.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.23.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.24.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.24.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.24.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.24.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.24.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.24.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.24.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.24.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.24.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.25.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.25.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.25.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.25.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.25.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.25.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.25.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.25.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.25.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.26.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.26.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.26.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.26.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.26.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.26.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.26.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.26.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.26.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.27.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.27.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.27.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.27.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.27.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.27.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.27.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.27.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.27.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.28.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.28.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.28.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.28.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.28.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.28.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.28.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.28.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.28.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.29.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.29.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.29.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.29.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.29.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
367146385408 params in total.
734292770816 bytes in total.
7335.48 sec, 53.18 sec, 95.46 MB/s
-----------------------------------------------------------------------------

[160/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00006-of-00022.pth
   0 :  109051904 : layers.29.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.29.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.29.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.29.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.30.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.30.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.30.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.30.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.30.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.30.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.30.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.30.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.30.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.31.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.31.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.31.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.31.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.31.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.31.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.31.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.31.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.31.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.32.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.32.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.32.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.32.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.32.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.32.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.32.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.32.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.32.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.33.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.33.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.33.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.33.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.33.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.33.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.33.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.33.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.33.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.34.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.34.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.34.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.34.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.34.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.34.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.34.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.34.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.34.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.35.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.35.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.35.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.35.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.35.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
369537335296 params in total.
739074670592 bytes in total.
7388.73 sec, 53.25 sec, 95.39 MB/s
-----------------------------------------------------------------------------

[161/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00007-of-00022.pth
   0 :  109051904 : layers.35.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.35.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.35.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.35.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.36.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.36.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.36.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.36.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.36.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.36.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.36.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.36.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.36.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.37.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.37.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.37.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.37.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.37.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.37.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.37.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.37.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.37.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.38.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.38.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.38.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.38.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.38.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.38.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.38.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.38.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.38.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.39.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.39.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.39.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.39.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.39.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.39.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.39.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.39.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.39.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.40.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.40.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.40.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.40.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.40.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.40.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.40.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.40.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.40.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.41.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.41.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.41.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.41.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.41.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
371928285184 params in total.
743856570368 bytes in total.
7442.18 sec, 53.45 sec, 95.32 MB/s
-----------------------------------------------------------------------------

[162/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00008-of-00022.pth
   0 :  109051904 : layers.41.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.41.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.41.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.41.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.42.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.42.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.42.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.42.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.42.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.42.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.42.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.42.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.42.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.43.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.43.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.43.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.43.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.43.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.43.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.43.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.43.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.43.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.44.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.44.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.44.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.44.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.44.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.44.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.44.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.44.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.44.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.45.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.45.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.45.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.45.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.45.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.45.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.45.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.45.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.45.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.46.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.46.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.46.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.46.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.46.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.46.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.46.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.46.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.46.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.47.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.47.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.47.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.47.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.47.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
374319235072 params in total.
748638470144 bytes in total.
7495.88 sec, 53.70 sec, 95.25 MB/s
-----------------------------------------------------------------------------

[163/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00009-of-00022.pth
   0 :  109051904 : layers.47.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.47.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.47.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.47.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.48.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.48.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.48.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.48.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.48.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.48.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.48.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.48.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.48.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.49.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.49.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.49.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.49.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.49.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.49.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.49.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.49.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.49.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.50.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.50.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.50.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.50.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.50.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.50.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.50.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.50.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.50.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.51.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.51.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.51.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.51.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.51.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.51.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.51.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.51.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.51.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.52.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.52.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.52.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.52.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.52.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.52.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.52.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.52.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.52.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.53.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.53.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.53.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.53.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.53.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
376710184960 params in total.
753420369920 bytes in total.
7549.83 sec, 53.95 sec, 95.17 MB/s
-----------------------------------------------------------------------------

[164/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00010-of-00022.pth
   0 :  109051904 : layers.53.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.53.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.53.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.53.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.54.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.54.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.54.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.54.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.54.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.54.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.54.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.54.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.54.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.55.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.55.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.55.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.55.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.55.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.55.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.55.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.55.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.55.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.56.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.56.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.56.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.56.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.56.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.56.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.56.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.56.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.56.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.57.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.57.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.57.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.57.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.57.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.57.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.57.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.57.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.57.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.58.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.58.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.58.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.58.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.58.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.58.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.58.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.58.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.58.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.59.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.59.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.59.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.59.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.59.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
379101134848 params in total.
758202269696 bytes in total.
7604.23 sec, 54.40 sec, 95.09 MB/s
-----------------------------------------------------------------------------

[165/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00011-of-00022.pth
   0 :  109051904 : layers.59.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.59.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.59.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.59.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.60.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.60.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.60.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.60.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.60.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.60.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.60.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.60.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.60.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.61.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.61.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.61.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.61.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.61.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.61.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.61.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.61.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.61.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.62.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.62.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.62.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.62.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.62.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.62.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.62.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.62.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.62.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.63.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.63.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.63.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.63.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.63.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.63.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.63.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.63.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.63.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.64.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.64.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.64.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.64.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.64.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.64.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.64.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.64.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.64.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.65.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.65.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.65.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.65.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.65.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
381492084736 params in total.
762984169472 bytes in total.
7658.19 sec, 53.96 sec, 95.01 MB/s
-----------------------------------------------------------------------------

[166/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00012-of-00022.pth
   0 :  109051904 : layers.65.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.65.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.65.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.65.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.66.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.66.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.66.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.66.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.66.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.66.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.66.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.66.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.66.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.67.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.67.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.67.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.67.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.67.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.67.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.67.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.67.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.67.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.68.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.68.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.68.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.68.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.68.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.68.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.68.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.68.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.68.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.69.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.69.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.69.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.69.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.69.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.69.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.69.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.69.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.69.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.70.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.70.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.70.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.70.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.70.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.70.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.70.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.70.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.70.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.71.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.71.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.71.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.71.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.71.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
383883034624 params in total.
767766069248 bytes in total.
7712.05 sec, 53.86 sec, 94.94 MB/s
-----------------------------------------------------------------------------

[167/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00013-of-00022.pth
   0 :  109051904 : layers.71.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.71.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.71.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.71.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.72.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.72.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.72.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.72.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.72.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.72.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.72.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.72.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.72.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.73.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.73.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.73.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.73.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.73.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.73.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.73.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.73.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.73.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.74.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.74.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.74.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.74.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.74.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.74.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.74.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.74.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.74.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.75.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.75.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.75.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.75.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.75.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.75.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.75.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.75.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.75.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.76.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.76.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.76.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.76.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.76.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.76.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.76.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.76.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.76.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.77.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.77.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.77.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.77.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.77.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
386273984512 params in total.
772547969024 bytes in total.
7765.99 sec, 53.94 sec, 94.87 MB/s
-----------------------------------------------------------------------------

[168/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00014-of-00022.pth
   0 :  109051904 : layers.77.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.77.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.77.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.77.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.78.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.78.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.78.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.78.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.78.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.78.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.78.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.78.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.78.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.79.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.79.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.79.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.79.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.79.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.79.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.79.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.79.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.79.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.80.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.80.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.80.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.80.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.80.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.80.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.80.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.80.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.80.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.81.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.81.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.81.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.81.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.81.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.81.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.81.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.81.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.81.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.82.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.82.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.82.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.82.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.82.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.82.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.82.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.82.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.82.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.83.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.83.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.83.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.83.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.83.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
388664934400 params in total.
777329868800 bytes in total.
7819.61 sec, 53.62 sec, 94.80 MB/s
-----------------------------------------------------------------------------

[169/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00015-of-00022.pth
   0 :  109051904 : layers.83.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.83.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.83.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.83.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.84.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.84.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.84.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.84.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.84.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.84.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.84.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.84.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.84.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.85.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.85.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.85.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.85.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.85.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.85.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.85.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.85.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.85.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.86.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.86.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.86.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.86.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.86.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.86.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.86.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.86.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.86.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.87.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.87.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.87.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.87.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.87.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.87.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.87.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.87.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.87.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.88.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.88.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.88.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.88.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.88.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.88.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.88.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.88.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.88.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.89.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.89.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.89.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.89.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.89.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
391055884288 params in total.
782111768576 bytes in total.
7873.20 sec, 53.59 sec, 94.74 MB/s
-----------------------------------------------------------------------------

[170/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00016-of-00022.pth
   0 :  109051904 : layers.89.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.89.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.89.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.89.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.90.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.90.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.90.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.90.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.90.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.90.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.90.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.90.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.90.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.91.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.91.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.91.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.91.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.91.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.91.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.91.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.91.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.91.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.92.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.92.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.92.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.92.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.92.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.92.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.92.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.92.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.92.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.93.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.93.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.93.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.93.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.93.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.93.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.93.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.93.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.93.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.94.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.94.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.94.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.94.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.94.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.94.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.94.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.94.attention_norm.weight     : [16384]         : torch.bfloat16
  48 :      16384 : layers.94.ffn_norm.weight           : [16384]         : torch.bfloat16
  49 :   33554432 : layers.95.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.95.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.95.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.95.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.95.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
393446834176 params in total.
786893668352 bytes in total.
7927.27 sec, 54.07 sec, 94.67 MB/s
-----------------------------------------------------------------------------

[171/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00017-of-00022.pth
   0 :  109051904 : layers.95.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.95.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.95.attention_norm.weight     : [16384]         : torch.bfloat16
   3 :      16384 : layers.95.ffn_norm.weight           : [16384]         : torch.bfloat16
   4 :   33554432 : layers.96.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.96.attention.wk.weight       : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.96.attention.wv.weight       : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.96.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.96.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.96.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.96.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.96.attention_norm.weight     : [16384]         : torch.bfloat16
  12 :      16384 : layers.96.ffn_norm.weight           : [16384]         : torch.bfloat16
  13 :   33554432 : layers.97.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.97.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.97.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.97.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.97.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.97.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.97.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.97.attention_norm.weight     : [16384]         : torch.bfloat16
  21 :      16384 : layers.97.ffn_norm.weight           : [16384]         : torch.bfloat16
  22 :   33554432 : layers.98.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.98.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.98.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.98.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.98.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.98.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.98.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.98.attention_norm.weight     : [16384]         : torch.bfloat16
  30 :      16384 : layers.98.ffn_norm.weight           : [16384]         : torch.bfloat16
  31 :   33554432 : layers.99.attention.wq.weight       : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.99.attention.wk.weight       : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.99.attention.wv.weight       : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.99.attention.wo.weight       : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.99.feed_forward.w1.weight    : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.99.feed_forward.w3.weight    : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.99.feed_forward.w2.weight    : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.99.attention_norm.weight     : [16384]         : torch.bfloat16
  39 :      16384 : layers.99.ffn_norm.weight           : [16384]         : torch.bfloat16
  40 :   33554432 : layers.100.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.100.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.100.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.100.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.100.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.100.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.100.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.100.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.100.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.101.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.101.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.101.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.101.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.101.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
395837784064 params in total.
791675568128 bytes in total.
7981.95 sec, 54.68 sec, 94.59 MB/s
-----------------------------------------------------------------------------

[172/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00018-of-00022.pth
   0 :  109051904 : layers.101.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.101.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.101.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.101.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.102.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.102.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.102.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.102.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.102.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.102.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.102.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.102.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.102.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.103.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.103.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.103.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.103.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.103.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.103.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.103.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.103.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.103.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.104.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.104.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.104.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.104.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.104.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.104.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.104.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.104.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.104.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.105.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.105.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.105.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.105.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.105.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.105.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.105.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.105.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.105.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.106.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.106.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.106.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.106.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.106.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.106.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.106.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.106.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.106.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.107.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.107.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.107.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.107.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.107.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
398228733952 params in total.
796457467904 bytes in total.
8036.46 sec, 54.51 sec, 94.51 MB/s
-----------------------------------------------------------------------------

[173/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00019-of-00022.pth
   0 :  109051904 : layers.107.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.107.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.107.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.107.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.108.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.108.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.108.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.108.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.108.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.108.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.108.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.108.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.108.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.109.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.109.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.109.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.109.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.109.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.109.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.109.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.109.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.109.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.110.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.110.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.110.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.110.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.110.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.110.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.110.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.110.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.110.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.111.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.111.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.111.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.111.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.111.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.111.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.111.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.111.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.111.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.112.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.112.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.112.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.112.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.112.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.112.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.112.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.112.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.112.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.113.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.113.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.113.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.113.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.113.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
400619683840 params in total.
801239367680 bytes in total.
8090.97 sec, 54.52 sec, 94.44 MB/s
-----------------------------------------------------------------------------

[174/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00020-of-00022.pth
   0 :  109051904 : layers.113.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.113.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.113.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.113.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.114.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.114.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.114.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.114.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.114.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.114.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.114.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.114.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.114.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.115.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.115.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.115.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.115.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.115.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.115.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.115.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.115.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.115.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.116.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.116.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.116.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.116.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.116.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.116.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.116.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.116.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.116.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.117.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.117.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.117.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.117.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.117.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.117.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.117.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.117.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.117.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.118.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.118.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.118.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.118.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.118.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.118.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.118.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.118.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.118.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.119.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.119.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.119.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.119.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.119.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
403010633728 params in total.
806021267456 bytes in total.
8145.50 sec, 54.52 sec, 94.37 MB/s
-----------------------------------------------------------------------------

[175/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00021-of-00022.pth
   0 :  109051904 : layers.119.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.119.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.119.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.119.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :   33554432 : layers.120.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
   5 :    2097152 : layers.120.attention.wk.weight      : [128, 16384]    : torch.bfloat16
   6 :    2097152 : layers.120.attention.wv.weight      : [128, 16384]    : torch.bfloat16
   7 :   33554432 : layers.120.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
   8 :  109051904 : layers.120.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
   9 :  109051904 : layers.120.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  10 :  109051904 : layers.120.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  11 :      16384 : layers.120.attention_norm.weight    : [16384]         : torch.bfloat16
  12 :      16384 : layers.120.ffn_norm.weight          : [16384]         : torch.bfloat16
  13 :   33554432 : layers.121.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  14 :    2097152 : layers.121.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  15 :    2097152 : layers.121.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  16 :   33554432 : layers.121.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  17 :  109051904 : layers.121.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  18 :  109051904 : layers.121.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  19 :  109051904 : layers.121.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  20 :      16384 : layers.121.attention_norm.weight    : [16384]         : torch.bfloat16
  21 :      16384 : layers.121.ffn_norm.weight          : [16384]         : torch.bfloat16
  22 :   33554432 : layers.122.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  23 :    2097152 : layers.122.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  24 :    2097152 : layers.122.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  25 :   33554432 : layers.122.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  26 :  109051904 : layers.122.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  27 :  109051904 : layers.122.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  28 :  109051904 : layers.122.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  29 :      16384 : layers.122.attention_norm.weight    : [16384]         : torch.bfloat16
  30 :      16384 : layers.122.ffn_norm.weight          : [16384]         : torch.bfloat16
  31 :   33554432 : layers.123.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  32 :    2097152 : layers.123.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  33 :    2097152 : layers.123.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  34 :   33554432 : layers.123.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  35 :  109051904 : layers.123.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  36 :  109051904 : layers.123.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  37 :  109051904 : layers.123.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  38 :      16384 : layers.123.attention_norm.weight    : [16384]         : torch.bfloat16
  39 :      16384 : layers.123.ffn_norm.weight          : [16384]         : torch.bfloat16
  40 :   33554432 : layers.124.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  41 :    2097152 : layers.124.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  42 :    2097152 : layers.124.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  43 :   33554432 : layers.124.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  44 :  109051904 : layers.124.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16
  45 :  109051904 : layers.124.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
  46 :  109051904 : layers.124.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
  47 :      16384 : layers.124.attention_norm.weight    : [16384]         : torch.bfloat16
  48 :      16384 : layers.124.ffn_norm.weight          : [16384]         : torch.bfloat16
  49 :   33554432 : layers.125.attention.wq.weight      : [2048, 16384]   : torch.bfloat16
  50 :    2097152 : layers.125.attention.wk.weight      : [128, 16384]    : torch.bfloat16
  51 :    2097152 : layers.125.attention.wv.weight      : [128, 16384]    : torch.bfloat16
  52 :   33554432 : layers.125.attention.wo.weight      : [16384, 2048]   : torch.bfloat16
  53 :  109051904 : layers.125.feed_forward.w1.weight   : [6656, 16384]   : torch.bfloat16

-----------------------------------------------------------------------------
405401583616 params in total.
810803167232 bytes in total.
8199.60 sec, 54.11 sec, 94.30 MB/s
-----------------------------------------------------------------------------

[176/176]: Loading models/Meta-Llama-3.1-405B-Instruct/mp8/consolidated.07/consolidated-00022-of-00022.pth
   0 :  109051904 : layers.125.feed_forward.w3.weight   : [6656, 16384]   : torch.bfloat16
   1 :  109051904 : layers.125.feed_forward.w2.weight   : [16384, 6656]   : torch.bfloat16
   2 :      16384 : layers.125.attention_norm.weight    : [16384]         : torch.bfloat16
   3 :      16384 : layers.125.ffn_norm.weight          : [16384]         : torch.bfloat16
   4 :      16384 : norm.weight                         : [16384]         : torch.bfloat16
   5 :  262668288 : output.weight                       : [16032, 16384]  : torch.bfloat16
Total number of parameters: 405882404864
405.8824 B
811764809728 Bytes
756.0149 GB
